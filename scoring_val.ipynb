{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scoring_val.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/shopee/blob/main/scoring_val.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h757Lh3cLDbF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywtK1vOfrNDD"
      },
      "source": [
        "def f1_score(y_true, y_pred):\n",
        "    y_true = y_true.apply(lambda x: set(x.split()))\n",
        "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
        "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
        "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
        "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
        "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
        "    return f1\n",
        "def get_neighbors(df, embeddings, KNN = 50, image = True):\n",
        "    model = NearestNeighbors(n_neighbors = KNN)\n",
        "    model.fit(embeddings)\n",
        "    distances, indices = model.kneighbors(embeddings)\n",
        "    \n",
        "    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n",
        "    thresholds = list(np.arange(4.0, 6.0, 0.1))\n",
        "    scores = []\n",
        "    for threshold in thresholds:\n",
        "            predictions = []\n",
        "            for k in range(embeddings.shape[0]):\n",
        "                idx = np.where(distances[k,] < threshold)[0]\n",
        "                ids = indices[k,idx]\n",
        "                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n",
        "                predictions.append(posting_ids)\n",
        "            df['pred_matches'] = predictions\n",
        "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
        "            score = df['f1'].mean()\n",
        "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
        "            scores.append(score)\n",
        "    thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
        "    max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
        "    best_threshold = max_score['thresholds'].values[0]\n",
        "    best_score = max_score['scores'].values[0]\n",
        "    print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
        "    del model, distances, indices\n",
        "    gc.collect()\n",
        "\n",
        "class check(Callback):\n",
        "    def __init__(self, X_seq):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.X_seq = X_seq\n",
        "    \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict_generator(self.X_seq, steps=len(self.X_seq), \n",
        "                                                            use_multiprocessing=False, workers=1, \n",
        "                                                            max_queue_size=2*4)\n",
        "            get_neighbors(self.df,y_pred)\n",
        "            \n",
        "kappa_metric = KappaEvaluationSeq(val_seq, Y, Y2, smpls, 'val')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM8bO_S3o9_F"
      },
      "source": [
        "from tensorflow.keras.callbacks import *\n",
        "from sklearn.neighbors import *\n",
        "from sklearn.preprocessing import *\n",
        "class stocasticensembling(Callback):\n",
        "  def __init__(self):\n",
        "    #save_se_weights: save after each epoch ?\n",
        "\n",
        "    self.lr_start   = 0.000001\n",
        "    self.lr_max     = 0.000005 * BATCH_SIZE\n",
        "    self.lr_min     = 0.000001\n",
        "    self.lr_ramp_ep = 10\n",
        "    self.lr_sus_ep  = 0\n",
        "    self.model_count = 0\n",
        "    self.clr_iterations = 0\n",
        "    self.lr_decay   = 0.8\n",
        "    self.cycle_len=2\n",
        "    self.swa_cycle_start_inx=45\n",
        "    self.iter_per_epoch=107\n",
        "    self.alpha1=alpha1=self.lr_min\n",
        "    self.iter_per_cycle = self.cycle_len * self.iter_per_epoch\n",
        "    self.alpha2=alpha2=self.lr_min*10\n",
        "    self.cycle_num=0\n",
        "\n",
        "  def on_train_end(self,logs={}):\n",
        "    self.weight_update()\n",
        "    self.model.set_weights(self.swa_weights)\n",
        "    \n",
        "  \n",
        "  \n",
        "  def on_epoch_end(self,epoch,logs=None):\n",
        "    self.cycle_num+=1\n",
        "    if (self._t_cycle() !=1) or (epoch<self.swa_cycle_start_inx):\n",
        "      return\n",
        "    self.weight_update()\n",
        "    self.model_count+=1\n",
        "  \n",
        "  \n",
        "  def on_batch_begin(self,batch,logs=None):\n",
        "    if self.cycle_num<self.swa_cycle_start_inx:\n",
        "      self.clr_iterations+=1\n",
        "      lr=self._clr_schedule()\n",
        "      K.set_value(self.model.optimizer.lr,lr)\n",
        "  \n",
        "  \n",
        "  \n",
        "  def weight_update(self):\n",
        "    weights=self.model.get_weights()\n",
        "    if self.model_count==0:\n",
        "      self.swa_weights=weights\n",
        "    for i in range(0,len(weights)):\n",
        "      self.swa_weights[i]=(self.swa_weights[i]*self.model_count+weights[i])/(self.model_count+1)\n",
        "  \n",
        "  \n",
        "  def _t_cycle(self):\n",
        "        return (((self.clr_iterations - 1) % self.iter_per_cycle) + 1) / self.iter_per_cycle\n",
        "  \n",
        "  \n",
        "  def _clr_schedule(self):\n",
        "    if self.cycle_num>self.swa_cycle_start_inx:\n",
        "      return ((1.0 - 1.0 *self._t_cycle()) * self.alpha2) + (1.0 *self._t_cycle() *self.alpha1)\n",
        "    else:\n",
        "      if self.cycle_num < self.lr_ramp_ep:\n",
        "            lr = (self.lr_max - self.lr_start) / self.lr_ramp_ep * self.cycle_num + self.lr_start   \n",
        "      elif self.cycle_num < self.lr_ramp_ep + self.lr_sus_ep:\n",
        "            lr = self.lr_max    \n",
        "      else:\n",
        "            lr = (self.lr_max - self.lr_min) * self.lr_decay**(self.cycle_num - self.lr_ramp_ep - self.lr_sus_ep) + self.lr_min   \n",
        "      return lr\n",
        "\n",
        "# Arcmarginproduct class keras layer\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.reshape(y,(-1,1))\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "#         m = tf.reshape(dm,(-1,1))\n",
        "#         self.m = m\n",
        "#         self.cos_m = tf.math.cos(m)\n",
        "#         self.sin_m = tf.math.sin(m)\n",
        "#         self.th = tf.math.cos(math.pi - m)\n",
        "#         self.mm = tf.math.sin(math.pi - m) * m\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        one_hot=one_hot[:,0,:]\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        outpu = tf.reshape(output,(-1,N_CLASSES))\n",
        "        return output\n",
        "\n",
        "class GeMPoolingLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, p=1., train_p=False):\n",
        "        super().__init__()\n",
        "        self.p = tf.Variable(p, dtype=tf.float32)\n",
        "        self.eps = 1e-6\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, **kwargs):\n",
        "        inputs = tf.clip_by_value(inputs, clip_value_min=1e-6, clip_value_max=tf.reduce_max(inputs))\n",
        "        inputs = tf.pow(inputs, self.p)\n",
        "        inputs = tf.reduce_mean(inputs, axis=[1, 2], keepdims=False)\n",
        "        inputs = tf.pow(inputs, 1./self.p)\n",
        "        return inputs\n",
        "\n",
        "# Function to create our EfficientNetB3 model\n",
        "def get_model():\n",
        "\n",
        "    with strategy.scope():\n",
        "\n",
        "        margin = ArcMarginProduct(\n",
        "            n_classes = N_CLASSES, \n",
        "            s = 30, \n",
        "            m = 0.5, \n",
        "            name='head/arc_margin', \n",
        "            dtype='float32'\n",
        "            )\n",
        "\n",
        "        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
        "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "        x = efn.EfficientNetB2(weights = 'imagenet', include_top = False)(inp)\n",
        "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "#         x = tf.keras.layers.Dropout(0.25)(x)\n",
        "#         x = tf.keras.layers.Dense(512)(x)\n",
        "#         x = tf.keras.layers.BatchNormalization()(x)\n",
        "#         x = tf.keras.layers.PReLU()(x)\n",
        "        x = margin([x, label])\n",
        "        \n",
        "        output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "\n",
        "        model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "            ) \n",
        "        \n",
        "        return model\n",
        "def f1_score(y_true, y_pred):\n",
        "    y_true = y_true.apply(lambda x: set(x.split()))\n",
        "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
        "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
        "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
        "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
        "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
        "    return f1\n",
        "def get_neighbors(df, embeddings, KNN = 50, image = True):\n",
        "    model = NearestNeighbors(n_neighbors = KNN)\n",
        "    model.fit(embeddings)\n",
        "    distances, indices = model.kneighbors(embeddings)\n",
        "    \n",
        "    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n",
        "    thresholds = list(np.arange(4.0, 6.0, 0.1))\n",
        "    scores = []\n",
        "    for threshold in thresholds:\n",
        "            predictions = []\n",
        "            for k in range(embeddings.shape[0]):\n",
        "                idx = np.where(distances[k,] < threshold)[0]\n",
        "                ids = indices[k,idx]\n",
        "                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n",
        "                predictions.append(posting_ids)\n",
        "            df['pred_matches'] = predictions\n",
        "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
        "            score = df['f1'].mean()\n",
        "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
        "            scores.append(score)\n",
        "    thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
        "    max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
        "    best_threshold = max_score['thresholds'].values[0]\n",
        "    best_score = max_score['scores'].values[0]\n",
        "    print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
        "    del model, distances, indices\n",
        "    gc.collect()\n",
        "\n",
        "class check(Callback):\n",
        "    def __init__(self, X_seq,df,interval):\n",
        "        super(Callback, self).__init__()\n",
        "        self.df=df\n",
        "        self.X_seq = X_seq\n",
        "        self.interval=interval\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict_generator(self.X_seq)[:self.df.shape[0]]\n",
        "            get_neighbors(self.df,y_pred)\n",
        "            \n",
        "def train_and_evaluate(df,a):\n",
        "    train=pd.read_csv('../input/shopee-product-matching/train.csv')\n",
        "    train['label_group']=le.fit_transform(train['label_group'])\n",
        "    train=train.drop_duplicates('label_group').reset_index(drop=True)\n",
        "    train=train.loc[a]\n",
        "    df=df.loc[df['label_group'].isin(list(train['label_group'].values))]\n",
        "    print(df.shape)\n",
        "    # Seed everything\n",
        "    seed_everything(SEED)\n",
        "    \n",
        "    print('\\n')\n",
        "    print('-'*50)\n",
        "    train=TRAINING_FILENAMES\n",
        "    train_dataset = get_training_dataset(TRAINING_FILENAMES, ordered = False)\n",
        "    valid_dataset = get_validation_dataset(VALIDATION_FILENAMES)\n",
        "    STEPS_PER_EPOCH = count_data_items(train) // BATCH_SIZE\n",
        "    STEPS_PER_EPOCH_all = count_data_items(TRAINING_FILENAMES) // BATCH_SIZE\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = get_model()\n",
        "    se=stocasticensembling()\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'EfficientNetB7_{IMAGE_SIZE[0]}_{SEED}.h5', \n",
        "                                                    monitor = 'val_loss', \n",
        "                                                    save_best_only = True,\n",
        "                                                    save_weights_only = True, \n",
        "                                                    mode = 'min')\n",
        "    history = model.fit(train_dataset,\n",
        "                        steps_per_epoch = 1,\n",
        "                        epochs = 40,\n",
        "                        callbacks = [stocasticensembling(),check(valid_dataset,df,5)])\n",
        "\n",
        "    model.save_weights('weights.hdf5')\n",
        "a=np.load('../input/stratify-counts/test_4.npy')\n",
        "df=pd.read_csv('../input/shopee-product-matching/train.csv')\n",
        "tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
        "df['matches'] = df['label_group'].map(tmp)\n",
        "df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
        "df['label_group']=le.fit_transform(df['label_group'])\n",
        "train_and_evaluate(df,a)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}