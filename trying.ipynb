{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trying.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4cc53bc6af16437db3ef58df9f225efa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_41f6511606344e21a4e7766e1d4c8b4f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e4ca10a6197845deadfc6c7b56e42e8d",
              "IPY_MODEL_b524c25a78af472f85eb7b0c592836bf"
            ]
          }
        },
        "41f6511606344e21a4e7766e1d4c8b4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e4ca10a6197845deadfc6c7b56e42e8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6f1bf23a01884be2b9fea30a78052b87",
            "_dom_classes": [],
            "description": "Epoch [TRAIN] 1:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 4281,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b74b20eebcc462699a760d77ae84118"
          }
        },
        "b524c25a78af472f85eb7b0c592836bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_84baf419172847cc9dfb70f23d9f042e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/4281 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_82fb65b8d92242ef8ab95d503977db8d"
          }
        },
        "6f1bf23a01884be2b9fea30a78052b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b74b20eebcc462699a760d77ae84118": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84baf419172847cc9dfb70f23d9f042e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "82fb65b8d92242ef8ab95d503977db8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/shopee/blob/main/trying.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmisKMKfiWHb",
        "outputId": "6ff6b5ac-60e9-41fb-838c-1f6ccfcbbcf0"
      },
      "source": [
        "!git clone https://github.com/rwightman/pytorch-image-models.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-image-models'...\n",
            "remote: Enumerating objects: 6318, done.\u001b[K\n",
            "remote: Counting objects: 100% (495/495), done.\u001b[K\n",
            "remote: Compressing objects: 100% (206/206), done.\u001b[K\n",
            "remote: Total 6318 (delta 307), reused 414 (delta 283), pack-reused 5823\u001b[K\n",
            "Receiving objects: 100% (6318/6318), 17.10 MiB | 32.30 MiB/s, done.\n",
            "Resolving deltas: 100% (4581/4581), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ6UG0VkisDW",
        "outputId": "3c7dfa3e-78fc-43fa-ef1f-60ceacc7da44"
      },
      "source": [
        "cd /content/pytorch-image-models"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/pytorch-image-models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_63MzUji7eb",
        "outputId": "8d3bf5b2-dd98-447f-ff17-7cba4e192005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dBXnhXmnCjm",
        "outputId": "79ee6812-96b1-4bec-bbf4-f91a09c57fac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"tapaskd123\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"aba8dc1f085221111d925003fe5a88ed\" # key from the json file\n",
        "!kaggle competitions download -c shopee-product-matching"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading 0006c8e5462ae52167402bac1c2e916e.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/55.9k [00:00<?, ?B/s]\n",
            "100% 55.9k/55.9k [00:00<00:00, 84.8MB/s]\n",
            "Downloading 0008377d3662e83ef44e1881af38b879.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/24.9k [00:00<?, ?B/s]\n",
            "100% 24.9k/24.9k [00:00<00:00, 25.7MB/s]\n",
            "Downloading 0007585c4d0f932859339129f709bfdc.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/35.0k [00:00<?, ?B/s]\n",
            "100% 35.0k/35.0k [00:00<00:00, 31.9MB/s]\n",
            "Downloading 0014f61389cbaa687a58e38a97b6383d.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/60.0k [00:00<?, ?B/s]\n",
            "100% 60.0k/60.0k [00:00<00:00, 55.2MB/s]\n",
            "Downloading 001be52b2beec40ddc1d2d7fc7a68f08.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/44.4k [00:00<?, ?B/s]\n",
            "100% 44.4k/44.4k [00:00<00:00, 47.2MB/s]\n",
            "Downloading 0027aaf8dd8bdbf0e4f2c19024e436cf.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/33.7k [00:00<?, ?B/s]\n",
            "100% 33.7k/33.7k [00:00<00:00, 33.6MB/s]\n",
            "Downloading 001f5580b058c6b8e33132190a757318.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/28.5k [00:00<?, ?B/s]\n",
            "100% 28.5k/28.5k [00:00<00:00, 30.1MB/s]\n",
            "Downloading 0000a68812bc7e98c42888dfb1c07da0.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/118k [00:00<?, ?B/s]\n",
            "100% 118k/118k [00:00<00:00, 38.7MB/s]\n",
            "Downloading 00136d1cf4edede0203f32f05f660588.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/34.5k [00:00<?, ?B/s]\n",
            "100% 34.5k/34.5k [00:00<00:00, 35.3MB/s]\n",
            "Downloading 0019a3c6755a194cb2e2c12bfc63972e.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/20.6k [00:00<?, ?B/s]\n",
            "100% 20.6k/20.6k [00:00<00:00, 21.3MB/s]\n",
            "Downloading 001e11145b8e9bf5ac51110c0fdd8697.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/120k [00:00<?, ?B/s]\n",
            "100% 120k/120k [00:00<00:00, 39.0MB/s]\n",
            "Downloading 00286d2760e433a8a01cbd9e056144f7.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/74.1k [00:00<?, ?B/s]\n",
            "100% 74.1k/74.1k [00:00<00:00, 76.6MB/s]\n",
            "Downloading 00117e4fc239b1b641ff08340b429633.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/68.3k [00:00<?, ?B/s]\n",
            "100% 68.3k/68.3k [00:00<00:00, 69.3MB/s]\n",
            "Downloading 00324695e37299a00b955674d984592b.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/664k [00:00<?, ?B/s]\n",
            "100% 664k/664k [00:00<00:00, 92.8MB/s]\n",
            "Downloading 002039aaf8618627a0442d5e89e5dda6.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/23.4k [00:00<?, ?B/s]\n",
            "100% 23.4k/23.4k [00:00<00:00, 25.5MB/s]\n",
            "Downloading 00303ad1c062fdeaf5f41b9ffb71a5fb.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/24.1k [00:00<?, ?B/s]\n",
            "100% 24.1k/24.1k [00:00<00:00, 21.8MB/s]\n",
            "Downloading 000a190fdd715a2a36faed16e2c65df7.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/51.3k [00:00<?, ?B/s]\n",
            "100% 51.3k/51.3k [00:00<00:00, 50.9MB/s]\n",
            "Downloading 001f4c8331d0554d133b10d85b7fafb2.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/31.4k [00:00<?, ?B/s]\n",
            "100% 31.4k/31.4k [00:00<00:00, 32.3MB/s]\n",
            "Downloading 00039780dfc94d01db8676fe789ecd05.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/93.4k [00:00<?, ?B/s]\n",
            "100% 93.4k/93.4k [00:00<00:00, 95.2MB/s]\n",
            "Downloading 0013e7355ffc5ff8fb1ccad3e42d92fe.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/37.6k [00:00<?, ?B/s]\n",
            "100% 37.6k/37.6k [00:00<00:00, 33.3MB/s]\n",
            "Downloading 002f978c58a44a00aadfca71c3cad2bb.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/14.3k [00:00<?, ?B/s]\n",
            "100% 14.3k/14.3k [00:00<00:00, 12.6MB/s]\n",
            "Downloading 001d7f5d9a2fac714f4d5f37b3baffb4.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/40.4k [00:00<?, ?B/s]\n",
            "100% 40.4k/40.4k [00:00<00:00, 41.5MB/s]\n",
            "Downloading 00144a49c56599d45354a1c28104c039.jpg to /content/pytorch-image-models\n",
            "  0% 0.00/40.6k [00:00<?, ?B/s]\n",
            "100% 40.6k/40.6k [00:00<00:00, 41.5MB/s]\n",
            "Downloading train.csv.zip to /content/pytorch-image-models\n",
            "  0% 0.00/2.48M [00:00<?, ?B/s]\n",
            "100% 2.48M/2.48M [00:00<00:00, 168MB/s]\n",
            "Downloading sample_submission.csv to /content/pytorch-image-models\n",
            "  0% 0.00/115 [00:00<?, ?B/s]\n",
            "100% 115/115 [00:00<00:00, 119kB/s]\n",
            "Downloading test.csv to /content/pytorch-image-models\n",
            "  0% 0.00/456 [00:00<?, ?B/s]\n",
            "100% 456/456 [00:00<00:00, 402kB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRgIYMbVnCg4"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "import os \n",
        "import cv2 \n",
        "import timm \n",
        "\n",
        "import albumentations \n",
        "from albumentations.pytorch.transforms import ToTensor\n",
        "\n",
        "import torch \n",
        "import torch.nn.functional as F \n",
        "from torch import nn \n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "import math\n",
        "\n",
        "from tqdm.notebook import tqdm \n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAaTL3OinCeI",
        "outputId": "ff0a8cb6-adb3-4648-ad43-f2af2f8e251f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "4cc53bc6af16437db3ef58df9f225efa",
            "41f6511606344e21a4e7766e1d4c8b4f",
            "e4ca10a6197845deadfc6c7b56e42e8d",
            "b524c25a78af472f85eb7b0c592836bf",
            "6f1bf23a01884be2b9fea30a78052b87",
            "4b74b20eebcc462699a760d77ae84118",
            "84baf419172847cc9dfb70f23d9f042e",
            "82fb65b8d92242ef8ab95d503977db8d"
          ]
        }
      },
      "source": [
        "class Config:\n",
        "    \n",
        "    DATA_DIR = '/content/gdrive/My Drive/shopee/'\n",
        "    TRAIN_CSV = '/content/pytorch-image-models/train.csv.zip'\n",
        "\n",
        "    IMG_SIZE = 512\n",
        "    MEAN = [0.485, 0.456, 0.406]\n",
        "    STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "    EPOCHS = 15  # Try 15 epochs\n",
        "    BATCH_SIZE = 8\n",
        "\n",
        "    NUM_WORKERS = 2\n",
        "    DEVICE = 'cuda'\n",
        "\n",
        "    CLASSES = 11014 \n",
        "    SCALE = 30 \n",
        "    MARGIN = 0.5\n",
        "\n",
        "    MODEL_NAME = 'eca_nfnet_l0'\n",
        "    FC_DIM = 512\n",
        "    SCHEDULER_PARAMS = {\n",
        "            \"lr_start\": 1e-5,\n",
        "            \"lr_max\": 1e-5 * 32,\n",
        "            \"lr_min\": 1e-6,\n",
        "            \"lr_ramp_ep\": 5,\n",
        "            \"lr_sus_ep\": 0,\n",
        "            \"lr_decay\": 0.8,\n",
        "        }\n",
        "class ShopeeDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self,df, transform = None):\n",
        "        self.df = df \n",
        "        self.root_dir = Config.DATA_DIR\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        img_path = os.path.join(self.root_dir,row.image)\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        label = row.label_group\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "\n",
        "        return {\n",
        "            'image' : image,\n",
        "            'label' : torch.tensor(label).long()\n",
        "        }\n",
        "def get_train_transforms():\n",
        "    return albumentations.Compose(\n",
        "        [   \n",
        "            albumentations.Resize(Config.IMG_SIZE,Config.IMG_SIZE,always_apply=True),\n",
        "            albumentations.HorizontalFlip(p=0.5),\n",
        "            albumentations.VerticalFlip(p=0.5),\n",
        "            albumentations.Rotate(limit=120, p=0.8),\n",
        "            albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n",
        "            albumentations.Normalize(mean = Config.MEAN, std = Config.STD),\n",
        "            ToTensor(),\n",
        "        ]\n",
        "    )\n",
        "#credit : https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images?scriptVersionId=58269290&cellId=22\n",
        "\n",
        "class ShopeeScheduler(_LRScheduler):\n",
        "    def __init__(self, optimizer, lr_start=5e-6, lr_max=1e-5,\n",
        "                 lr_min=1e-6, lr_ramp_ep=5, lr_sus_ep=0, lr_decay=0.8,\n",
        "                 last_epoch=-1):\n",
        "        self.lr_start = lr_start\n",
        "        self.lr_max = lr_max\n",
        "        self.lr_min = lr_min\n",
        "        self.lr_ramp_ep = lr_ramp_ep\n",
        "        self.lr_sus_ep = lr_sus_ep\n",
        "        self.lr_decay = lr_decay\n",
        "        super(ShopeeScheduler, self).__init__(optimizer, last_epoch)\n",
        "        \n",
        "    def get_lr(self):\n",
        "        if not self._get_lr_called_within_step:\n",
        "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
        "                          \"please use `get_last_lr()`.\", UserWarning)\n",
        "        \n",
        "        if self.last_epoch == 0:\n",
        "            self.last_epoch += 1\n",
        "            return [self.lr_start for _ in self.optimizer.param_groups]\n",
        "        \n",
        "        lr = self._compute_lr_from_epoch()\n",
        "        self.last_epoch += 1\n",
        "        \n",
        "        return [lr for _ in self.optimizer.param_groups]\n",
        "    \n",
        "    def _get_closed_form_lr(self):\n",
        "        return self.base_lrs\n",
        "    \n",
        "    def _compute_lr_from_epoch(self):\n",
        "        if self.last_epoch < self.lr_ramp_ep:\n",
        "            lr = ((self.lr_max - self.lr_start) / \n",
        "                  self.lr_ramp_ep * self.last_epoch + \n",
        "                  self.lr_start)\n",
        "        \n",
        "        elif self.last_epoch < self.lr_ramp_ep + self.lr_sus_ep:\n",
        "            lr = self.lr_max\n",
        "            \n",
        "        else:\n",
        "            lr = ((self.lr_max - self.lr_min) * self.lr_decay**\n",
        "                  (self.last_epoch - self.lr_ramp_ep - self.lr_sus_ep) + \n",
        "                  self.lr_min)\n",
        "        return lr\n",
        "#credit : https://github.com/Yonghongwei/Gradient-Centralization\n",
        "\n",
        "def centralized_gradient(x, use_gc=True, gc_conv_only=False):\n",
        "    if use_gc:\n",
        "        if gc_conv_only:\n",
        "            if len(list(x.size())) > 3:\n",
        "                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n",
        "        else:\n",
        "            if len(list(x.size())) > 1:\n",
        "                x.add_(-x.mean(dim=tuple(range(1, len(list(x.size())))), keepdim=True))\n",
        "    return x\n",
        "\n",
        "\n",
        "class Ranger(Optimizer):\n",
        "\n",
        "    def __init__(self, params, lr=1e-3,                       # lr\n",
        "                 alpha=0.5, k=5, N_sma_threshhold=5,           # Ranger options\n",
        "                 betas=(.95, 0.999), eps=1e-5, weight_decay=0,  # Adam options\n",
        "                 # Gradient centralization on or off, applied to conv layers only or conv + fc layers\n",
        "                 use_gc=True, gc_conv_only=False, gc_loc=True\n",
        "                 ):\n",
        "\n",
        "        # parameter checks\n",
        "        if not 0.0 <= alpha <= 1.0:\n",
        "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
        "        if not 1 <= k:\n",
        "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
        "        if not lr > 0:\n",
        "            raise ValueError(f'Invalid Learning Rate: {lr}')\n",
        "        if not eps > 0:\n",
        "            raise ValueError(f'Invalid eps: {eps}')\n",
        "\n",
        "        # parameter comments:\n",
        "        # beta1 (momentum) of .95 seems to work better than .90...\n",
        "        # N_sma_threshold of 5 seems better in testing than 4.\n",
        "        # In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n",
        "\n",
        "        # prep defaults and init torch.optim base\n",
        "        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas,\n",
        "                        N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "        # adjustable threshold\n",
        "        self.N_sma_threshhold = N_sma_threshhold\n",
        "\n",
        "        # look ahead params\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "\n",
        "        # radam buffer for state\n",
        "        self.radam_buffer = [[None, None, None] for ind in range(10)]\n",
        "\n",
        "        # gc on or off\n",
        "        self.gc_loc = gc_loc\n",
        "        self.use_gc = use_gc\n",
        "        self.gc_conv_only = gc_conv_only\n",
        "        # level of gradient centralization\n",
        "        #self.gc_gradient_threshold = 3 if gc_conv_only else 1\n",
        "\n",
        "        print(\n",
        "            f\"Ranger optimizer loaded. \\nGradient Centralization usage = {self.use_gc}\")\n",
        "        if (self.use_gc and self.gc_conv_only == False):\n",
        "            print(f\"GC applied to both conv and fc layers\")\n",
        "        elif (self.use_gc and self.gc_conv_only == True):\n",
        "            print(f\"GC applied to conv layers only\")\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        print(\"set state called\")\n",
        "        super(Ranger, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        # note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.\n",
        "        # Uncomment if you need to use the actual closure...\n",
        "\n",
        "        # if closure is not None:\n",
        "        #loss = closure()\n",
        "\n",
        "        # Evaluate averages and grad, update param tensors\n",
        "        for group in self.param_groups:\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data.float()\n",
        "\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\n",
        "                        'Ranger optimizer does not support sparse gradients')\n",
        "\n",
        "                p_data_fp32 = p.data.float()\n",
        "\n",
        "                state = self.state[p]  # get state dict for this param\n",
        "\n",
        "                if len(state) == 0:  # if first time to run...init dictionary with our desired entries\n",
        "                    # if self.first_run_check==0:\n",
        "                    # self.first_run_check=1\n",
        "                    #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
        "\n",
        "                    # look ahead weight storage now in state dict\n",
        "                    state['slow_buffer'] = torch.empty_like(p.data)\n",
        "                    state['slow_buffer'].copy_(p.data)\n",
        "\n",
        "                else:\n",
        "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
        "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(\n",
        "                        p_data_fp32)\n",
        "\n",
        "                # begin computations\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "\n",
        "                # GC operation for Conv layers and FC layers\n",
        "                # if grad.dim() > self.gc_gradient_threshold:\n",
        "                #    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))\n",
        "                if self.gc_loc:\n",
        "                    grad = centralized_gradient(grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # compute variance mov avg\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "\n",
        "                # compute mean moving avg\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "\n",
        "                buffered = self.radam_buffer[int(state['step'] % 10)]\n",
        "\n",
        "                if state['step'] == buffered[0]:\n",
        "                    N_sma, step_size = buffered[1], buffered[2]\n",
        "                else:\n",
        "                    buffered[0] = state['step']\n",
        "                    beta2_t = beta2 ** state['step']\n",
        "                    N_sma_max = 2 / (1 - beta2) - 1\n",
        "                    N_sma = N_sma_max - 2 * \\\n",
        "                        state['step'] * beta2_t / (1 - beta2_t)\n",
        "                    buffered[1] = N_sma\n",
        "                    if N_sma > self.N_sma_threshhold:\n",
        "                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (\n",
        "                            N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
        "                    else:\n",
        "                        step_size = 1.0 / (1 - beta1 ** state['step'])\n",
        "                    buffered[2] = step_size\n",
        "\n",
        "                # if group['weight_decay'] != 0:\n",
        "                #    p_data_fp32.add_(-group['weight_decay']\n",
        "                #                     * group['lr'], p_data_fp32)\n",
        "\n",
        "                # apply lr\n",
        "                if N_sma > self.N_sma_threshhold:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    G_grad = exp_avg / denom\n",
        "                else:\n",
        "                    G_grad = exp_avg\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    G_grad.add_(p_data_fp32, alpha=group['weight_decay'])\n",
        "                # GC operation\n",
        "                if self.gc_loc == False:\n",
        "                    G_grad = centralized_gradient(G_grad, use_gc=self.use_gc, gc_conv_only=self.gc_conv_only)\n",
        "\n",
        "                p_data_fp32.add_(G_grad, alpha=-step_size * group['lr'])\n",
        "                p.data.copy_(p_data_fp32)\n",
        "\n",
        "                # integrated look ahead...\n",
        "                # we do it at the param level instead of group level\n",
        "                if state['step'] % group['k'] == 0:\n",
        "                    # get access to slow param tensor\n",
        "                    slow_p = state['slow_buffer']\n",
        "                    # (fast weights - slow weights) * alpha\n",
        "                    slow_p.add_(p.data - slow_p, alpha=self.alpha)\n",
        "                    # copy interpolated weights to RAdam param tensor\n",
        "                    p.data.copy_(slow_p)\n",
        "\n",
        "        return loss\n",
        "class Mish_func(torch.autograd.Function):\n",
        "    \n",
        "    @staticmethod\n",
        "    def forward(ctx, i):\n",
        "        result = i * torch.tanh(F.softplus(i))\n",
        "        ctx.save_for_backward(i)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        i = ctx.saved_tensors[0]\n",
        "  \n",
        "        v = 1. + i.exp()\n",
        "        h = v.log() \n",
        "        grad_gh = 1./h.cosh().pow_(2) \n",
        "\n",
        "        # Note that grad_hv * grad_vx = sigmoid(x)\n",
        "        #grad_hv = 1./v  \n",
        "        #grad_vx = i.exp()\n",
        "        \n",
        "        grad_hx = i.sigmoid()\n",
        "\n",
        "        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n",
        "        \n",
        "        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n",
        "        \n",
        "        return grad_output * grad_f \n",
        "\n",
        "\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        print(\"Mish initialized\")\n",
        "        pass\n",
        "    def forward(self, input_tensor):\n",
        "        return Mish_func.apply(input_tensor)\n",
        "def replace_activations(model, existing_layer, new_layer):\n",
        "    for name, module in reversed(model._modules.items()):\n",
        "        if len(list(module.children())) > 0:\n",
        "            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n",
        "\n",
        "        if type(module) == existing_layer:\n",
        "            layer_old = module\n",
        "            layer_new = new_layer\n",
        "            model._modules[name] = layer_new\n",
        "    return model\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
        "        super(ArcMarginProduct, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.scale = scale\n",
        "        self.margin = margin\n",
        "        self.ls_eps = ls_eps  # label smoothing\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(margin)\n",
        "        self.sin_m = math.sin(margin)\n",
        "        self.th = math.cos(math.pi - margin)\n",
        "        self.mm = math.sin(math.pi - margin) * margin\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        # --------------------------- convert label to one-hot ---------------------------\n",
        "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
        "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.scale\n",
        "\n",
        "        return output, nn.CrossEntropyLoss()(output,label)\n",
        "\n",
        "class ShopeeModel(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes = Config.CLASSES,\n",
        "        model_name = Config.MODEL_NAME,\n",
        "        fc_dim = Config.FC_DIM,\n",
        "        margin = Config.MARGIN,\n",
        "        scale = Config.SCALE,\n",
        "        use_fc = True,\n",
        "        pretrained = True):\n",
        "\n",
        "\n",
        "        super(ShopeeModel,self).__init__()\n",
        "        print('Building Model Backbone for {} model'.format(model_name))\n",
        "\n",
        "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
        "\n",
        "        if model_name == 'resnext50_32x4d':\n",
        "            final_in_features = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()\n",
        "            self.backbone.global_pool = nn.Identity()\n",
        "\n",
        "        elif 'efficientnet' in model_name:\n",
        "            final_in_features = self.backbone.classifier.in_features\n",
        "            self.backbone.classifier = nn.Identity()\n",
        "            self.backbone.global_pool = nn.Identity()\n",
        "        \n",
        "        elif 'nfnet' in model_name:\n",
        "            final_in_features = self.backbone.head.fc.in_features\n",
        "            self.backbone.head.fc = nn.Identity()\n",
        "            self.backbone.head.global_pool = nn.Identity()\n",
        "\n",
        "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.use_fc = use_fc\n",
        "\n",
        "        if use_fc:\n",
        "            self.dropout = nn.Dropout(p=0.0)\n",
        "            self.fc = nn.Linear(final_in_features, fc_dim)\n",
        "            self.bn = nn.BatchNorm1d(fc_dim)\n",
        "            self._init_params()\n",
        "            final_in_features = fc_dim\n",
        "\n",
        "        self.final = ArcMarginProduct(\n",
        "            final_in_features,\n",
        "            n_classes,\n",
        "            scale = scale,\n",
        "            margin = margin,\n",
        "            easy_margin = False,\n",
        "            ls_eps = 0.0\n",
        "        )\n",
        "\n",
        "    def _init_params(self):\n",
        "        nn.init.xavier_normal_(self.fc.weight)\n",
        "        nn.init.constant_(self.fc.bias, 0)\n",
        "        nn.init.constant_(self.bn.weight, 1)\n",
        "        nn.init.constant_(self.bn.bias, 0)\n",
        "\n",
        "    def forward(self, image, label):\n",
        "        feature = self.extract_feat(image)\n",
        "        logits = self.final(feature,label)\n",
        "        return logits\n",
        "\n",
        "    def extract_feat(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.backbone(x)\n",
        "        x = self.pooling(x).view(batch_size, -1)\n",
        "\n",
        "        if self.use_fc:\n",
        "            x = self.dropout(x)\n",
        "            x = self.fc(x)\n",
        "            x = self.bn(x)\n",
        "        return x\n",
        "def train_fn(model, data_loader, optimizer, scheduler, i):\n",
        "    model.train()\n",
        "    fin_loss = 0.0\n",
        "    tk = tqdm(data_loader, desc = \"Epoch\" + \" [TRAIN] \" + str(i+1))\n",
        "\n",
        "    for t,data in enumerate(tk):\n",
        "        for k,v in data.items():\n",
        "            data[k] = v.to(Config.DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        _, loss = model(**data)\n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "        fin_loss += loss.item() \n",
        "\n",
        "        tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1)), 'LR' : optimizer.param_groups[0]['lr']})\n",
        "    scheduler.step()\n",
        "\n",
        "    return fin_loss / len(data_loader)\n",
        "\n",
        "def eval_fn(model, data_loader, i):\n",
        "    model.eval()\n",
        "    fin_loss = 0.0\n",
        "    tk = tqdm(data_loader, desc = \"Epoch\" + \" [VALID] \" + str(i+1))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for t,data in enumerate(tk):\n",
        "            for k,v in data.items():\n",
        "                data[k] = v.to(Config.DEVICE)\n",
        "            _, loss = model(**data)\n",
        "            fin_loss += loss.item() \n",
        "\n",
        "            tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1))})\n",
        "        return fin_loss / len(data_loader)\n",
        "def run_training():\n",
        "    print('1')\n",
        "    df = pd.read_csv(Config.TRAIN_CSV)\n",
        "    print('1')\n",
        "\n",
        "    labelencoder= LabelEncoder()\n",
        "    print('1')\n",
        "    df['label_group'] = labelencoder.fit_transform(df['label_group'])\n",
        "    \n",
        "    trainset = ShopeeDataset(df, transform = get_train_transforms())\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        trainset,\n",
        "        batch_size = Config.BATCH_SIZE,\n",
        "        pin_memory = True,\n",
        "        num_workers = Config.NUM_WORKERS,\n",
        "        shuffle = True,\n",
        "        drop_last = True\n",
        "    )\n",
        "\n",
        "    model = ShopeeModel()\n",
        "    model.to(Config.DEVICE)\n",
        "    \n",
        "    \n",
        "    existing_layer = torch.nn.SiLU\n",
        "    new_layer = Mish()\n",
        "    model = replace_activations(model, existing_layer, new_layer) # in eca_nfnet_l0 SiLU() is used, but it will be replace by Mish()\n",
        "    \n",
        "    optimizer = Ranger(model.parameters(), lr = Config.SCHEDULER_PARAMS['lr_start'])\n",
        "    scheduler = ShopeeScheduler(optimizer,**Config.SCHEDULER_PARAMS)\n",
        "\n",
        "    for i in range(Config.EPOCHS):\n",
        "\n",
        "        avg_loss_train = train_fn(model, trainloader, optimizer, scheduler, i)\n",
        "    torch.save(model.state_dict(),'arcface_512x512_nfnet_l0(mish).pt')\n",
        "\n",
        "run_training()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "1\n",
            "1\n",
            "Building Model Backbone for eca_nfnet_l0 model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l0_ra2-e3e9ac50.pth\" to /root/.cache/torch/hub/checkpoints/ecanfnet_l0_ra2-e3e9ac50.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mish initialized\n",
            "Ranger optimizer loaded. \n",
            "Gradient Centralization usage = True\n",
            "GC applied to both conv and fc layers\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cc53bc6af16437db3ef58df9f225efa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Epoch [TRAIN] 1', max=4281.0, style=ProgressStyle(descripâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFDuIhdnnCbs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co-JhbvnnCZM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBS1JIVlnCWQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2VxKFePnCTe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}