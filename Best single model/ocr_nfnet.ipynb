{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ocr_nfnet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVdgUNEjPqcF6pB2WO6TSp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/shopee/blob/main/ocr_nfnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxU7chZVTbLj"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
        "\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "\tlayers = []\n",
        "\tin_channels = 3\n",
        "\tfor v in cfg:\n",
        "\t\tif v == 'M':\n",
        "\t\t\tlayers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "\t\telse:\n",
        "\t\t\tconv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "\t\t\tif batch_norm:\n",
        "\t\t\t\tlayers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "\t\t\telse:\n",
        "\t\t\t\tlayers += [conv2d, nn.ReLU(inplace=True)]\n",
        "\t\t\tin_channels = v\n",
        "\treturn nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\tdef __init__(self, features):\n",
        "\t\tsuper(VGG, self).__init__()\n",
        "\t\tself.features = features\n",
        "\t\tself.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "\t\tself.classifier = nn.Sequential(\n",
        "\t\t\tnn.Linear(512 * 7 * 7, 4096),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\tnn.Dropout(),\n",
        "\t\t\tnn.Linear(4096, 4096),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\tnn.Dropout(),\n",
        "\t\t\tnn.Linear(4096, 1000),\n",
        "\t\t)\n",
        "\n",
        "\t\tfor m in self.modules():\n",
        "\t\t\tif isinstance(m, nn.Conv2d):\n",
        "\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\t\t\t\tif m.bias is not None:\n",
        "\t\t\t\t\tnn.init.constant_(m.bias, 0)\n",
        "\t\t\telif isinstance(m, nn.BatchNorm2d):\n",
        "\t\t\t\tnn.init.constant_(m.weight, 1)\n",
        "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
        "\t\t\telif isinstance(m, nn.Linear):\n",
        "\t\t\t\tnn.init.normal_(m.weight, 0, 0.01)\n",
        "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.features(x)\n",
        "\t\tx = self.avgpool(x)\n",
        "\t\tx = x.view(x.size(0), -1)\n",
        "\t\tx = self.classifier(x)\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "class extractor(nn.Module):\n",
        "\tdef __init__(self, pretrained):\n",
        "\t\tsuper(extractor, self).__init__()\n",
        "\t\tvgg16_bn = VGG(make_layers(cfg, batch_norm=True))\n",
        "\t\tif pretrained:\n",
        "\t\t\tvgg16_bn.load_state_dict(torch.load('./pths/vgg16_bn-6c64b313.pth'))\n",
        "\t\tself.features = vgg16_bn.features\n",
        "\t\n",
        "\tdef forward(self, x):\n",
        "\t\tout = []\n",
        "\t\tfor m in self.features:\n",
        "\t\t\tx = m(x)\n",
        "\t\t\tif isinstance(m, nn.MaxPool2d):\n",
        "\t\t\t\tout.append(x)\n",
        "\t\treturn out[1:]\n",
        "\n",
        "\n",
        "class merge(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(merge, self).__init__()\n",
        "\n",
        "\t\tself.conv1 = nn.Conv2d(1024, 128, 1)\n",
        "\t\tself.bn1 = nn.BatchNorm2d(128)\n",
        "\t\tself.relu1 = nn.ReLU()\n",
        "\t\tself.conv2 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "\t\tself.bn2 = nn.BatchNorm2d(128)\n",
        "\t\tself.relu2 = nn.ReLU()\n",
        "\n",
        "\t\tself.conv3 = nn.Conv2d(384, 64, 1)\n",
        "\t\tself.bn3 = nn.BatchNorm2d(64)\n",
        "\t\tself.relu3 = nn.ReLU()\n",
        "\t\tself.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "\t\tself.bn4 = nn.BatchNorm2d(64)\n",
        "\t\tself.relu4 = nn.ReLU()\n",
        "\n",
        "\t\tself.conv5 = nn.Conv2d(192, 32, 1)\n",
        "\t\tself.bn5 = nn.BatchNorm2d(32)\n",
        "\t\tself.relu5 = nn.ReLU()\n",
        "\t\tself.conv6 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "\t\tself.bn6 = nn.BatchNorm2d(32)\n",
        "\t\tself.relu6 = nn.ReLU()\n",
        "\n",
        "\t\tself.conv7 = nn.Conv2d(32, 32, 3, padding=1)\n",
        "\t\tself.bn7 = nn.BatchNorm2d(32)\n",
        "\t\tself.relu7 = nn.ReLU()\n",
        "\t\t\n",
        "\t\tfor m in self.modules():\n",
        "\t\t\tif isinstance(m, nn.Conv2d):\n",
        "\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\t\t\t\tif m.bias is not None:\n",
        "\t\t\t\t\tnn.init.constant_(m.bias, 0)\n",
        "\t\t\telif isinstance(m, nn.BatchNorm2d):\n",
        "\t\t\t\tnn.init.constant_(m.weight, 1)\n",
        "\t\t\t\tnn.init.constant_(m.bias, 0)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\ty = F.interpolate(x[3], scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\t\ty = torch.cat((y, x[2]), 1)\n",
        "\t\ty = self.relu1(self.bn1(self.conv1(y)))\t\t\n",
        "\t\ty = self.relu2(self.bn2(self.conv2(y)))\n",
        "\t\t\n",
        "\t\ty = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\t\ty = torch.cat((y, x[1]), 1)\n",
        "\t\ty = self.relu3(self.bn3(self.conv3(y)))\t\t\n",
        "\t\ty = self.relu4(self.bn4(self.conv4(y)))\n",
        "\t\t\n",
        "\t\ty = F.interpolate(y, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\t\ty = torch.cat((y, x[0]), 1)\n",
        "\t\ty = self.relu5(self.bn5(self.conv5(y)))\t\t\n",
        "\t\ty = self.relu6(self.bn6(self.conv6(y)))\n",
        "\t\t\n",
        "\t\ty = self.relu7(self.bn7(self.conv7(y)))\n",
        "\t\treturn y\n",
        "\n",
        "class output(nn.Module):\n",
        "\tdef __init__(self, scope=512):\n",
        "\t\tsuper(output, self).__init__()\n",
        "\t\tself.conv1 = nn.Conv2d(32, 1, 1)\n",
        "\t\tself.sigmoid1 = nn.Sigmoid()\n",
        "\t\tself.conv2 = nn.Conv2d(32, 4, 1)\n",
        "\t\tself.sigmoid2 = nn.Sigmoid()\n",
        "\t\tself.conv3 = nn.Conv2d(32, 1, 1)\n",
        "\t\tself.sigmoid3 = nn.Sigmoid()\n",
        "\t\tself.scope = 512\n",
        "\t\tfor m in self.modules():\n",
        "\t\t\tif isinstance(m, nn.Conv2d):\n",
        "\t\t\t\tnn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "\t\t\t\tif m.bias is not None:\n",
        "\t\t\t\t\tnn.init.constant_(m.bias, 0)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tscore = self.sigmoid1(self.conv1(x))\n",
        "\t\tloc   = self.sigmoid2(self.conv2(x)) * self.scope\n",
        "\t\tangle = (self.sigmoid3(self.conv3(x)) - 0.5) * math.pi\n",
        "\t\tgeo   = torch.cat((loc, angle), 1) \n",
        "\t\treturn score, geo\n",
        "\t\t\n",
        "\t\n",
        "class EAST(nn.Module):\n",
        "\tdef __init__(self, pretrained=True):\n",
        "\t\tsuper(EAST, self).__init__()\n",
        "\t\tself.extractor = extractor(pretrained)\n",
        "\t\tself.merge     = merge()\n",
        "\t\tself.output    = output()\n",
        "\t\n",
        "\tdef forward(self, x):\n",
        "\t\treturn self.output(self.merge(self.extractor(x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2SSk-wIUxtK"
      },
      "source": [
        "import sys\n",
        "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "import math\n",
        "import random \n",
        "import os \n",
        "import cv2\n",
        "import timm\n",
        "\n",
        "from tqdm import tqdm \n",
        "\n",
        "import albumentations as A \n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "import torch \n",
        "from torch.utils.data import Dataset \n",
        "from torch import nn\n",
        "import torch.nn.functional as F \n",
        "\n",
        "import gc\n",
        "import cudf\n",
        "import cuml\n",
        "import cupy\n",
        "from cuml.feature_extraction.text import TfidfVectorizer\n",
        "from cuml.neighbors import NearestNeighbors\n",
        "class CFG:\n",
        "    \n",
        "    img_size = 512\n",
        "    batch_size = 12\n",
        "    seed = 2020\n",
        "    \n",
        "    device = 'cuda'\n",
        "    classes = 11014\n",
        "    \n",
        "    model_name = 'eca_nfnet_l0'\n",
        "    model_path = '../input/fold-0-731/arcface_512x512_nfnet_l0(mish).pt'\n",
        "    \n",
        "    scale = 30 \n",
        "    margin = 0.5\n",
        "def read_dataset():\n",
        "    df = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
        "    df_cu = cudf.DataFrame(df)\n",
        "    image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n",
        "    return df, df_cu, image_paths\n",
        "def seed_torch(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    \n",
        "seed_torch(CFG.seed)\n",
        "def combine_predictions(row):\n",
        "    x = np.concatenate([row['image_predictions'], row['text_predictions']])\n",
        "    return ' '.join( np.unique(x))\n",
        "def get_image_predictions(df, embeddings,threshold = 0.0):\n",
        "    \n",
        "    if len(df) > 3:\n",
        "        KNN = 50\n",
        "    else : \n",
        "        KNN = 3\n",
        "    \n",
        "    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n",
        "    model.fit(embeddings)\n",
        "    distances, indices = model.kneighbors(embeddings)\n",
        "    \n",
        "    predictions = []\n",
        "    for k in tqdm(range(embeddings.shape[0])):\n",
        "        idx = np.where(distances[k,] < threshold)[0]\n",
        "        ids = indices[k,idx]\n",
        "        posting_ids = df['posting_id'].iloc[ids].values\n",
        "        predictions.append(posting_ids)\n",
        "        \n",
        "    del model, distances, indices\n",
        "    gc.collect()\n",
        "    return predictions\n",
        "def get_test_transforms():\n",
        "\n",
        "    return A.Compose(\n",
        "        [\n",
        "            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n",
        "            A.Normalize(),\n",
        "        ToTensorV2(p=1.0)\n",
        "        ]\n",
        "    )\n",
        "class ShopeeDataset(Dataset):\n",
        "    def __init__(self, image_paths, transforms=None):\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "        self.augmentations = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.image_paths.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_path = self.image_paths[index]\n",
        "        \n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        if self.augmentations:\n",
        "            augmented = self.augmentations(image=image)\n",
        "            image = augmented['image']       \n",
        "    \n",
        "        return image\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
        "        super(ArcMarginProduct, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.scale = scale\n",
        "        self.margin = margin\n",
        "        self.ls_eps = ls_eps  # label smoothing\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(margin)\n",
        "        self.sin_m = math.sin(margin)\n",
        "        self.th = math.cos(math.pi - margin)\n",
        "        self.mm = math.sin(math.pi - margin) * margin\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        # --------------------------- convert label to one-hot ---------------------------\n",
        "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
        "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.scale\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class ShopeeModel(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name,\n",
        "#         ocr,\n",
        "        n_classes = 11014,\n",
        "        fc_dim = 512,\n",
        "        margin = 0.5,\n",
        "        scale = 30,\n",
        "        use_fc = True,\n",
        "        pretrained = False):\n",
        "\n",
        "\n",
        "        super(ShopeeModel,self).__init__()\n",
        "        print('Building Model Backbone for {} model'.format(model_name))\n",
        "\n",
        "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
        "\n",
        "        if model_name == 'resnext50_32x4d':\n",
        "            final_in_features = self.backbone.fc.in_features\n",
        "            self.backbone.fc = nn.Identity()\n",
        "            self.backbone.global_pool = nn.Identity()\n",
        "\n",
        "        elif 'efficientnet' in model_name:\n",
        "            final_in_features = self.backbone.classifier.in_features\n",
        "            self.backbone.classifier = nn.Identity()\n",
        "            self.backbone.global_pool = nn.Identity()\n",
        "        \n",
        "        elif 'nfnet' in model_name:\n",
        "            final_in_features = self.backbone.head.fc.in_features\n",
        "            self.backbone.head.fc = nn.Identity()\n",
        "            self.backbone.head.global_pool = nn.Identity()\n",
        "\n",
        "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
        "#         self.ocr = ocr\n",
        "        self.use_fc = use_fc\n",
        "\n",
        "        if use_fc:\n",
        "            self.dropout = nn.Dropout(p=0.0)\n",
        "            self.fc = nn.Linear(final_in_features, fc_dim)\n",
        "            self.bn = nn.BatchNorm1d(fc_dim)\n",
        "            self.cnv=nn.Conv2d(4,3,2)\n",
        "            self._init_params()\n",
        "            \n",
        "            final_in_features = fc_dim\n",
        "\n",
        "        self.final = ArcMarginProduct(\n",
        "            final_in_features,\n",
        "            n_classes,\n",
        "            scale = scale,\n",
        "            margin = margin,\n",
        "            easy_margin = False,\n",
        "            ls_eps = 0.0\n",
        "        )\n",
        "\n",
        "    def _init_params(self):\n",
        "        nn.init.xavier_normal_(self.fc.weight)\n",
        "        nn.init.constant_(self.fc.bias, 0)\n",
        "        nn.init.constant_(self.bn.weight, 1)\n",
        "        nn.init.constant_(self.bn.bias, 0)\n",
        "\n",
        "    def forward(self, image):\n",
        "        feature = self.extract_feat(image)\n",
        "#         logits = self.final(feature,label)\n",
        "        return feature\n",
        "\n",
        "    def extract_feat(self, x):\n",
        "#         o=self.ocr(x)[0]\n",
        "        \n",
        "        x = self.cnv(x)\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.backbone(x)\n",
        "        x = self.pooling(x).view(batch_size, -1)\n",
        "\n",
        "        if self.use_fc:\n",
        "            x = self.dropout(x)\n",
        "            x = self.fc(x)\n",
        "            x = self.bn(x)\n",
        "        return x\n",
        "class TotModel(nn.Module):\n",
        "    def __init__(self,back,ocr):\n",
        "        super(TotModel,self).__init__()\n",
        "        self.ocr=ocr\n",
        "        self.back=back\n",
        "        self.up=torch.nn.Upsample(size=(CFG.img_size,CFG.img_size))\n",
        "    def forward(self, image):\n",
        "        ocr = self.ocr(image)[0]\n",
        "        ocr=self.up(ocr)\n",
        "        image=torch.cat((image,ocr),1)\n",
        "        feature = self.back(image)\n",
        "        return feature\n",
        "class Mish_func(torch.autograd.Function):\n",
        "    \n",
        "    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def forward(ctx, i):\n",
        "        result = i * torch.tanh(F.softplus(i))\n",
        "        ctx.save_for_backward(i)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        i = ctx.saved_variables[0]\n",
        "  \n",
        "        v = 1. + i.exp()\n",
        "        h = v.log() \n",
        "        grad_gh = 1./h.cosh().pow_(2) \n",
        "\n",
        "        # Note that grad_hv * grad_vx = sigmoid(x)\n",
        "        #grad_hv = 1./v  \n",
        "        #grad_vx = i.exp()\n",
        "        \n",
        "        grad_hx = i.sigmoid()\n",
        "\n",
        "        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n",
        "        \n",
        "        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n",
        "        \n",
        "        return grad_output * grad_f \n",
        "\n",
        "\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        pass\n",
        "    def forward(self, input_tensor):\n",
        "        return Mish_func.apply(input_tensor)\n",
        "\n",
        "\n",
        "def replace_activations(model, existing_layer, new_layer):\n",
        "    \n",
        "    \"\"\"A function for replacing existing activation layers\"\"\"\n",
        "    \n",
        "    for name, module in reversed(model._modules.items()):\n",
        "        if len(list(module.children())) > 0:\n",
        "            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n",
        "\n",
        "        if type(module) == existing_layer:\n",
        "            layer_old = module\n",
        "            layer_new = new_layer\n",
        "            model._modules[name] = layer_new\n",
        "    return model\n",
        "def get_image_embeddings(image_paths, model_name = CFG.model_name):\n",
        "    embeds = []\n",
        "    \n",
        "    \n",
        "    model = ShopeeModel(model_name = model_name)\n",
        "    model.load_state_dict(torch.load('../input/shopee-pytorch-eca-nfnet-l0-image-training/arcface_512x512_nfnet_l0(17).pt')['state_dict'])\n",
        "    mod=EAST(pretrained=False)\n",
        "    mod.load_state_dict(torch.load('../input/east-pytorcu/east_vgg16.pth'))\n",
        "    model=TotModel(model,mod)\n",
        "    model.eval()\n",
        "    \n",
        "    \n",
        "    if model_name == 'eca_nfnet_l0':\n",
        "        model = replace_activations(model, torch.nn.SiLU, Mish())\n",
        "\n",
        "#     model.load_state_dict(torch.load(CFG.model_path))\n",
        "    model = model.to(CFG.device)\n",
        "    \n",
        "\n",
        "    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n",
        "    image_loader = torch.utils.data.DataLoader(\n",
        "        image_dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        pin_memory=True,\n",
        "        drop_last=False,\n",
        "        num_workers=4\n",
        "    )\n",
        "    \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for img in tqdm(image_loader): \n",
        "            img = img.cuda()\n",
        "            feat = model(img)\n",
        "            image_embeddings = feat.detach().cpu().numpy()\n",
        "            embeds.append(image_embeddings)\n",
        "    \n",
        "    \n",
        "    del model\n",
        "    image_embeddings = np.concatenate(embeds)\n",
        "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
        "    del embeds\n",
        "    gc.collect()\n",
        "    return image_embeddings\n",
        "def get_text_predictions(df, max_features = 25_000):\n",
        "    \n",
        "    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n",
        "    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n",
        "    preds = []\n",
        "    CHUNK = 1024*4\n",
        "\n",
        "    print('Finding similar titles...')\n",
        "    CTS = len(df)//CHUNK\n",
        "    if len(df)%CHUNK!=0: CTS += 1\n",
        "    for j in range( CTS ):\n",
        "\n",
        "        a = j*CHUNK\n",
        "        b = (j+1)*CHUNK\n",
        "        b = min(b,len(df))\n",
        "        print('chunk',a,'to',b)\n",
        "\n",
        "        # COSINE SIMILARITY DISTANCE\n",
        "        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n",
        "\n",
        "        for k in range(b-a):\n",
        "            IDX = cupy.where(cts[k,]>0.75)[0]\n",
        "            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
        "            preds.append(o)\n",
        "    \n",
        "    del model,text_embeddings\n",
        "    gc.collect()\n",
        "    return preds\n",
        "df,df_cu,image_paths = read_dataset()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD7Yd6gWUxqZ"
      },
      "source": [
        "image_embeddings = get_image_embeddings(image_paths.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx5YkOp5Uxng"
      },
      "source": [
        "\n",
        "image_predictions = get_image_predictions(df, image_embeddings, threshold = 0.25)\n",
        "text_predictions = get_text_predictions(df, max_features = 25_000)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_kY-NukWKqR"
      },
      "source": [
        "\n",
        "df['image_predictions'] = image_predictions\n",
        "df['text_predictions'] = text_predictions\n",
        "df['matches'] = df.apply(combine_predictions, axis = 1)\n",
        "df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
