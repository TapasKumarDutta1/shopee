{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ocr_full",
      "provenance": [],
      "authorship_tag": "ABX9TyP+ABhQ87S83rtgNKJwFdeH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/shopee/blob/main/ocr_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxU7chZVTbLj"
      },
      "source": [
        "from tensorflow.keras.models import load_model, model_from_json\n",
        "RESIZE_FACTOR = 2\n",
        "import tensorflow as tf\n",
        "json_file = open('../input/east-keras/model.json')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json, custom_objects={'tf': tf.compat.v1, 'RESIZE_FACTOR': RESIZE_FACTOR})\n",
        "model.load_weights('../input/east-keras/EAST_IC1513_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klY-MjqnYXXn"
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, df, img_size=512, batch_size=32, path=''): \n",
        "        self.df = df\n",
        "        self.img_size = img_size\n",
        "        self.batch_size = batch_size\n",
        "        self.path = path\n",
        "        self.indexes = np.arange( len(self.df) )\n",
        "        \n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        ct = len(self.df) // self.batch_size\n",
        "        ct += int(( (len(self.df)) % self.batch_size)!=0)\n",
        "        return ct\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        X = self.__data_generation(indexes)/255\n",
        "        return X\n",
        "            \n",
        "    def __data_generation(self, indexes):\n",
        "        'Generates data containing batch_size samples' \n",
        "        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n",
        "        df = self.df.iloc[indexes]\n",
        "        for i,(index,row) in enumerate(df.iterrows()):\n",
        "            img = cv2.imread(self.path+row.image)\n",
        "            X[i,] = cv2.resize(img,(self.img_size,self.img_size)) #/128.0 - 1.0\n",
        "        return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZRmNunOYXU3"
      },
      "source": [
        "\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FweraB-yYXSA"
      },
      "source": [
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "import tensorflow.keras.backend as K\n",
        "def mean(x):\n",
        "    x = K.mean(x, axis=-1, keepdims=True)\n",
        "    return x\n",
        "\n",
        "out=model.get_layer('pred_geo_map').output\n",
        "output=Lambda(mean,name='efi')(out)\n",
        "mod=Model(inputs=model.inputs,outputs=output)\n",
        "\n",
        "mod.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwnZqc-0YXPP"
      },
      "source": [
        "\n",
        "\n",
        "import cv2\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model, model_from_json\n",
        "\n",
        "import glob\n",
        "img_list=glob.glob('../input/shopee-product-matching/train_images/*')\n",
        "\n",
        "import pandas as pd\n",
        "test = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
        "\n",
        "import gc\n",
        "BASE = '../input/shopee-product-matching/train_images/'\n",
        "\n",
        "# WGT = '../input/shopee-efficientnetb6-arcmarginproduct/EfficientNetB6_512_42.h5'\n",
        "# model = efn.EfficientNetB6(weights=WGT,include_top=False, pooling='avg', input_shape=None)\n",
        "\n",
        "A = []\n",
        "B = []\n",
        "CHUNK = 1024*4\n",
        "\n",
        "print('Computing image embeddings...')\n",
        "CTS = len(test)//CHUNK\n",
        "if len(test)%CHUNK!=0: CTS += 1\n",
        "for i,j in enumerate( range( CTS ) ):\n",
        "    \n",
        "    a = j*CHUNK\n",
        "    b = (j+1)*CHUNK\n",
        "    b = min(b,len(test))\n",
        "    print('chunk',a,'to',b)\n",
        "    \n",
        "    test_gen = DataGenerator(test.iloc[a:b], batch_size=16, path=BASE)\n",
        "    a = mod.predict(test_gen,verbose=1,use_multiprocessing=True, workers=4)\n",
        "    A.append(a)\n",
        "#     B.append(b)\n",
        "    del test_gen\n",
        "    _ = gc.collect()\n",
        "del mod\n",
        "_ = gc.collect()\n",
        "image_embeddings = np.concatenate(A)\n",
        "# image_embeddings1 = np.concatenate(B)\n",
        "print('image embeddings shape',image_embeddings.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJTSJXqHYXMY"
      },
      "source": [
        "np.save('features.npy',image_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
