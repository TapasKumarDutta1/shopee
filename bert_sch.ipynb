{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_sch",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/shopee/blob/main/bert_sch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9922143b-037e-4fb5-854b-8cad36091be9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "from shutil import copyfile\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1dd6aaf-dfff-4b1c-eb21-bdbc4632ee54"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 30.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 22.4MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 25.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 24.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 27.0MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 18.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 19.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 18.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 18.6MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 18.6MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 18.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4"
      },
      "source": [
        "import tokenization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq"
      },
      "source": [
        "# Configuration\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 32\n",
        "# Seed\n",
        "SEED = 123\n",
        "# Verbosity\n",
        "VERBOSE = 1\n",
        "LR = 5e-5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe037ca-40b6-4e16-d400-b5fa26a37b8a"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "def read_and_preprocess():\n",
        "    df = pd.read_csv('/content/gdrive/My Drive/shopee/train.csv.zip')\n",
        "    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
        "    df['matches'] = df['label_group'].map(tmp)\n",
        "    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
        "    encoder = LabelEncoder()\n",
        "    df['label_group'] = encoder.fit_transform(df['label_group'])\n",
        "    N_CLASSES = df['label_group'].nunique()\n",
        "    print(f'We have {N_CLASSES} classes')\n",
        "    x_train, x_val, y_train, y_val = train_test_split(df[['title']], df['label_group'], shuffle = True, stratify = df['label_group'], random_state = SEED, test_size = 0.33)\n",
        "    return df, N_CLASSES, x_train, x_val, y_train, y_val\n",
        "\n",
        "# Return tokens, masks and segments from a text array or series\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
        "\n",
        "\n",
        "# Arcmarginproduct class keras layer\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n",
        "\n",
        "# Function to build bert model\n",
        "def build_bert_model(bert_layer, max_len = 512):\n",
        "    \n",
        "    margin = ArcMarginProduct(\n",
        "            n_classes = N_CLASSES, \n",
        "            s = 30, \n",
        "            m = 0.5, \n",
        "            name='head/arc_margin', \n",
        "            dtype='float32'\n",
        "            )\n",
        "    \n",
        "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    label = tf.keras.layers.Input(shape = (), name = 'label')\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    x = Dropout(0.3)(clf_output)\n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = PReLU()(x)\n",
        "    x = margin([x, label])\n",
        "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n",
        "                  loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "                  metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "    return model\n",
        "\n",
        "def load_train_and_evaluate(x_train, x_val, y_train, y_val):\n",
        "    seed_everything(SEED)\n",
        "    module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "    bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
        "\n",
        "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "    x_train = bert_encode(x_train['title'].values, tokenizer, max_len = 70)\n",
        "    x_val = bert_encode(x_val['title'].values, tokenizer, max_len = 70)\n",
        "    y_train = y_train.values\n",
        "    y_val = y_val.values\n",
        "    # Add targets to train and val\n",
        "    x_train = (x_train[0], x_train[1], x_train[2], y_train)\n",
        "    x_val = (x_val[0], x_val[1], x_val[2], y_val)\n",
        "    bert_model = build_bert_model(bert_layer, max_len = 70)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint('bert_inc.hdf5', \n",
        "                                                    monitor = 'val_loss', \n",
        "                                                    verbose = VERBOSE, \n",
        "                                                    save_best_only = True,\n",
        "                                                    save_weights_only = True, \n",
        "                                                    mode = 'min')\n",
        "    def get_lr_callback():\n",
        "      lr_start   = 0.00001\n",
        "      lr_max     = 0.00005 \n",
        "      lr_min     = 0.00001\n",
        "      lr_ramp_ep = 10\n",
        "      lr_sus_ep  = 0\n",
        "      lr_decay   = 0.8\n",
        "   \n",
        "      def lrfn(epoch):\n",
        "        if epoch < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max    \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
        "        return lr\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
        "    return lr_callback\n",
        "\n",
        "\n",
        "        lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
        "        return lr_callback\n",
        "    history = bert_model.fit(x_train, y_train,\n",
        "                             validation_data = (x_val, y_val),\n",
        "                             epochs = EPOCHS, \n",
        "                             callbacks = [checkpoint,get_lr_callback()],\n",
        "                             batch_size = BATCH_SIZE,\n",
        "                             verbose = VERBOSE)\n",
        "    \n",
        "    \n",
        "\n",
        "df, N_CLASSES, x_train, x_val, y_train, y_val = read_and_preprocess()\n",
        "load_train_and_evaluate(x_train, x_val, y_train, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 11014 classes\n",
            "Epoch 1/25\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "718/718 [==============================] - 1206s 2s/step - loss: 25.1002 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.7286 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 23.72864, saving model to bert_inc.hdf5\n",
            "Epoch 2/25\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00032800000000000006.\n",
            "521/718 [====================>.........] - ETA: 4:32 - loss: 23.6699 - sparse_categorical_accuracy: 0.0000e+00"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}