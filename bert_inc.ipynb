{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_inc",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/shopee/blob/main/bert_inc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9922143b-037e-4fb5-854b-8cad36091be9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "from shutil import copyfile\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1dd6aaf-dfff-4b1c-eb21-bdbc4632ee54"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 23.5MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 30.9MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 22.4MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 25.7MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 24.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 27.0MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 18.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 19.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 18.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 18.6MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 18.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 18.6MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 18.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 18.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mja2yCpAINM4"
      },
      "source": [
        "import tokenization"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo9D7_Mt01Qq"
      },
      "source": [
        "# Configuration\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 32\n",
        "# Seed\n",
        "SEED = 123\n",
        "# Verbosity\n",
        "VERBOSE = 1\n",
        "LR = 5e-5"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES4W36q1Kz7Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4bf15e49-ce8b-4947-c592-d1b8fc30d475"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "def read_and_preprocess():\n",
        "    df = pd.read_csv('/content/gdrive/My Drive/shopee/train.csv.zip')\n",
        "    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
        "    df['matches'] = df['label_group'].map(tmp)\n",
        "    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
        "    encoder = LabelEncoder()\n",
        "    df['label_group'] = encoder.fit_transform(df['label_group'])\n",
        "    N_CLASSES = df['label_group'].nunique()\n",
        "    print(f'We have {N_CLASSES} classes')\n",
        "    x_train, x_val, y_train, y_val = train_test_split(df[['title']], df['label_group'], shuffle = True, stratify = df['label_group'], random_state = SEED, test_size = 0.33)\n",
        "    return df, N_CLASSES, x_train, x_val, y_train, y_val\n",
        "\n",
        "# Return tokens, masks and segments from a text array or series\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
        "\n",
        "\n",
        "# Arcmarginproduct class keras layer\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n",
        "\n",
        "# Function to build bert model\n",
        "def build_bert_model(bert_layer, max_len = 512):\n",
        "    \n",
        "    margin = ArcMarginProduct(\n",
        "            n_classes = N_CLASSES, \n",
        "            s = 30, \n",
        "            m = 0.5, \n",
        "            name='head/arc_margin', \n",
        "            dtype='float32'\n",
        "            )\n",
        "    \n",
        "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    label = tf.keras.layers.Input(shape = (), name = 'label')\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    x = Dropout(0.3)(clf_output)\n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = PReLU()(x)\n",
        "    x = margin([x, label])\n",
        "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n",
        "                  loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "                  metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "    return model\n",
        "\n",
        "def load_train_and_evaluate(x_train, x_val, y_train, y_val):\n",
        "    seed_everything(SEED)\n",
        "    module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "    bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
        "\n",
        "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "    x_train = bert_encode(x_train['title'].values, tokenizer, max_len = 70)\n",
        "    x_val = bert_encode(x_val['title'].values, tokenizer, max_len = 70)\n",
        "    y_train = y_train.values\n",
        "    y_val = y_val.values\n",
        "    # Add targets to train and val\n",
        "    x_train = (x_train[0], x_train[1], x_train[2], y_train)\n",
        "    x_val = (x_val[0], x_val[1], x_val[2], y_val)\n",
        "    bert_model = build_bert_model(bert_layer, max_len = 70)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint('bert_inc.hdf5', \n",
        "                                                    monitor = 'val_loss', \n",
        "                                                    verbose = VERBOSE, \n",
        "                                                    save_best_only = True,\n",
        "                                                    save_weights_only = True, \n",
        "                                                    mode = 'min')\n",
        "    def get_lr_callback():\n",
        "   \n",
        "        def lrfn(epoch):\n",
        "            if epoch < 10:\n",
        "                lr = 1e-5  \n",
        "            elif epoch < 15:\n",
        "                lr = 2e-5  \n",
        "            elif epoch < 20:\n",
        "                lr = 3e-5  \n",
        "            elif epoch > 20:\n",
        "                lr = 4e-5    \n",
        "            return lr\n",
        "\n",
        "        lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
        "        return lr_callback\n",
        "    history = bert_model.fit(x_train, y_train,\n",
        "                             validation_data = (x_val, y_val),\n",
        "                             epochs = EPOCHS, \n",
        "                             callbacks = [checkpoint,get_lr_callback()],\n",
        "                             batch_size = BATCH_SIZE,\n",
        "                             verbose = VERBOSE)\n",
        "    \n",
        "    \n",
        "\n",
        "df, N_CLASSES, x_train, x_val, y_train, y_val = read_and_preprocess()\n",
        "load_train_and_evaluate(x_train, x_val, y_train, y_val)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 11014 classes\n",
            "Epoch 1/25\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "718/718 [==============================] - 1230s 2s/step - loss: 25.1362 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 24.1048 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 24.10483, saving model to /content/gdrive/MyDrive/shopee/bert_inc.hdf5\n",
            "Epoch 2/25\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "718/718 [==============================] - 1199s 2s/step - loss: 23.8145 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 23.3773 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_loss improved from 24.10483 to 23.37733, saving model to /content/gdrive/MyDrive/shopee/bert_inc.hdf5\n",
            "Epoch 3/25\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "718/718 [==============================] - 1161s 2s/step - loss: 22.9356 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 22.7617 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00003: val_loss improved from 23.37733 to 22.76174, saving model to /content/gdrive/MyDrive/shopee/bert_inc.hdf5\n",
            "Epoch 4/25\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "718/718 [==============================] - 1192s 2s/step - loss: 22.1994 - sparse_categorical_accuracy: 5.8579e-05 - val_loss: 22.1578 - val_sparse_categorical_accuracy: 0.0019\n",
            "\n",
            "Epoch 00004: val_loss improved from 22.76174 to 22.15778, saving model to /content/gdrive/MyDrive/shopee/bert_inc.hdf5\n",
            "Epoch 5/25\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "718/718 [==============================] - 1202s 2s/step - loss: 21.2410 - sparse_categorical_accuracy: 0.0015 - val_loss: 21.5117 - val_sparse_categorical_accuracy: 0.0050\n",
            "\n",
            "Epoch 00005: val_loss improved from 22.15778 to 21.51168, saving model to /content/gdrive/MyDrive/shopee/bert_inc.hdf5\n",
            "Epoch 6/25\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "718/718 [==============================] - 1203s 2s/step - loss: 20.0787 - sparse_categorical_accuracy: 0.0030 - val_loss: 20.7892 - val_sparse_categorical_accuracy: 0.0083\n",
            "\n",
            "Epoch 00006: val_loss improved from 21.51168 to 20.78918, saving model to /content/gdrive/MyDrive/shopee/bert_inc.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e52e04a22136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m \u001b[0mload_train_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-e52e04a22136>\u001b[0m in \u001b[0;36mload_train_and_evaluate\u001b[0;34m(x_train, x_val, y_train, y_val)\u001b[0m\n\u001b[1;32m    187\u001b[0m                              \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mget_lr_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                              \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                              verbose = VERBOSE)\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_should_save_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1416\u001b[0m                         'directory: {}'.format(filepath))\n\u001b[1;32m   1417\u001b[0m         \u001b[0;31m# Re-throw the error for any other causes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_file_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1392\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m                 self.model.save_weights(\n\u001b[0;32m-> 1394\u001b[0;31m                     filepath, overwrite=True, options=self._options)\n\u001b[0m\u001b[1;32m   1395\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[1;32m   2105\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2107\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2108\u001b[0m         \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/content/gdrive/MyDrive/shopee/bert_inc.hdf5', errno = 5, error message = 'Input/output error', flags = 13, o_flags = 242)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}