{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distil_swe.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOKXHXk0mpsvtgJWtwZ8QwL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/shopee/blob/main/distil_swe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxU7chZVTbLj"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.autonotebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import transformers\n",
        "from transformers import (BertTokenizer, BertModel,\n",
        "                          DistilBertTokenizer, DistilBertModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sLS0CSdY33i"
      },
      "source": [
        "\n",
        "\n",
        "le=LabelEncoder()\n",
        "a=np.load('../input/stratify-counts/train_0.npy')\n",
        "df=pd.read_csv('../input/shopee-product-matching/train.csv')\n",
        "df['label_group']=le.fit_transform(df['label_group'])\n",
        "df=df.drop_duplicates('label_group').reset_index(drop=True)\n",
        "labels=list(df.loc[a]['label_group'].values)\n",
        "train = pd.read_csv(\"../input/shopee-product-matching/train.csv\")\n",
        "\n",
        "labelencoder= LabelEncoder()\n",
        "train['label_group'] = labelencoder.fit_transform(train['label_group'])\n",
        "train=train.loc[train['label_group'].isin(labels)]\n",
        "train.shape    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahwfdhdhY30o"
      },
      "source": [
        "title_lengths = train['title'].apply(lambda x: len(x.split(\" \"))).to_numpy()\n",
        "print(f\"MIN words: {title_lengths.min()}, MAX words: {title_lengths.max()}\")\n",
        "plt.hist(title_lengths);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih_ev6ToY3xn"
      },
      "source": [
        "class CFG:\n",
        "    DistilBERT = True # if set to False, BERT model will be used\n",
        "    bert_hidden_size = 768\n",
        "    \n",
        "    batch_size = 64\n",
        "    epochs = 30\n",
        "    num_workers = 4\n",
        "    learning_rate = 1e-5 #3e-5\n",
        "    scheduler = \"ReduceLROnPlateau\"\n",
        "    step = 'epoch'\n",
        "    patience = 2\n",
        "    factor = 0.8\n",
        "    dropout = 0.5\n",
        "    model_path = \"/kaggle/working\"\n",
        "    max_length = 30\n",
        "    model_save_name = \"model.pt\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oee4oCeBY3u_"
      },
      "source": [
        "if CFG.DistilBERT:\n",
        "    model_name='cahya/distilbert-base-indonesian'\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "    bert_model = DistilBertModel.from_pretrained(model_name)\n",
        "else:\n",
        "    model_name='cahya/bert-base-indonesian-522M'\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    bert_model = BertModel.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VQ1AIcoY3sX"
      },
      "source": [
        "text = train['title'].values[np.random.randint(0, len(train) - 1, 1)[0]]\n",
        "print(f\"Text of the title: {text}\")\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "print(f\"Input tokens: {encoded_input['input_ids']}\")\n",
        "decoded_input = tokenizer.decode(encoded_input['input_ids'][0])\n",
        "print(f\"Decoded tokens: {decoded_input}\")\n",
        "output = bert_model(**encoded_input)\n",
        "print(f\"last layer's output shape: {output.last_hidden_state.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiDHkSJ8Y3ps"
      },
      "source": [
        "lbl_encoder = LabelEncoder()\n",
        "train['label_code'] = lbl_encoder.fit_transform(train['label_group'])\n",
        "NUM_CLASSES = train['label_code'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDd3h_E9Y3nC"
      },
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, mode=\"train\", max_length=None):\n",
        "        self.dataframe = dataframe\n",
        "        if mode != \"test\":\n",
        "            self.targets = dataframe['label_code'].values\n",
        "        texts = list(dataframe['title'].apply(lambda o: str(o)).values)\n",
        "        self.encodings = tokenizer(texts, \n",
        "                                   padding=True, \n",
        "                                   truncation=True, \n",
        "                                   max_length=max_length)\n",
        "        self.mode = mode\n",
        "        \n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n",
        "        if self.mode != \"test\":\n",
        "            item['labels'] = torch.tensor(self.targets[idx]).long()\n",
        "        return item\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "dataset = TextDataset(train.sample(1000), tokenizer, max_length=CFG.max_length)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, \n",
        "                                         batch_size=CFG.batch_size, \n",
        "                                         num_workers=CFG.num_workers, \n",
        "                                         shuffle=True)\n",
        "batch = next(iter(dataloader))\n",
        "print(batch['input_ids'].shape, batch['labels'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOecOpPrZFFn"
      },
      "source": [
        "class ArcMarginProduct(nn.Module):\n",
        "    r\"\"\"Implement of large margin arc distance: :\n",
        "        Args:\n",
        "            in_features: size of each input sample\n",
        "            out_features: size of each output sample\n",
        "            s: norm of input feature\n",
        "            m: margin\n",
        "            cos(theta + m)\n",
        "        \"\"\"\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
        "        super(ArcMarginProduct, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        # --------------------------- convert label to one-hot ---------------------------\n",
        "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
        "        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
        "        output *= self.s\n",
        "        # print(output)\n",
        "\n",
        "        return output\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, \n",
        "                 bert_model, \n",
        "                 num_classes=NUM_CLASSES, \n",
        "                 last_hidden_size=CFG.bert_hidden_size):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.bert_model = bert_model\n",
        "        self.arc_margin = ArcMarginProduct(last_hidden_size, \n",
        "                                           num_classes, \n",
        "                                           s=30.0, \n",
        "                                           m=0.50, \n",
        "                                           easy_margin=False)\n",
        "    \n",
        "    def get_bert_features(self, batch):\n",
        "        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "        last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)\n",
        "        CLS_token_state = last_hidden_state[:, 0, :] # obtaining CLS token state which is the first token.\n",
        "        return CLS_token_state\n",
        "    \n",
        "    def forward(self, batch):\n",
        "        CLS_hidden_state = self.get_bert_features(batch)\n",
        "        output = self.arc_margin(CLS_hidden_state, batch['labels'])\n",
        "        return output\n",
        "class AvgMeter:\n",
        "    def __init__(self, name=\"Metric\"):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.avg, self.sum, self.count = [0]*3\n",
        "    \n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "    \n",
        "    def __repr__(self):\n",
        "        text = f\"{self.name}: {self.avg:.4f}\"\n",
        "        return text\n",
        "def set_value(optimizer,lr):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return optimizer\n",
        "  \n",
        "def weight_update(model,model_count,swa_weights):\n",
        "    print('updating')\n",
        "    weights=model.state_dict()\n",
        "    if model_count==0:\n",
        "          swa_weights=weights\n",
        "          return swa_weights\n",
        "    for i in weights.keys():\n",
        "      swa_weights[i]=(swa_weights[i]*model_count+weights[i])/(model_count+1)\n",
        "    return swa_weights\n",
        "\n",
        "def _t_cycle(clr_iterations,iter_per_cycle):\n",
        "        return (((clr_iterations - 1) % iter_per_cycle) + 1) / iter_per_cycle\n",
        "  \n",
        "  \n",
        "def _clr_schedule(alpha2,alpha1,clr_iterations,iter_per_cycle,cycle_num):\n",
        "    swa_cycle_start_inx=40\n",
        "    \n",
        "\n",
        "    lr_start   = 0.000001\n",
        "    lr_max     = 0.000005 * 128\n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 5\n",
        "    lr_sus_ep  = 0\n",
        "    lr_decay   = 0.8\n",
        "    cycle_len=2\n",
        "\n",
        "\n",
        "    if cycle_num>=swa_cycle_start_inx:\n",
        "          return ((1.0 - 1.0 *_t_cycle(clr_iterations,iter_per_cycle)) * alpha2) + (1.0 *_t_cycle(clr_iterations,iter_per_cycle) *alpha1)\n",
        "    else:\n",
        "          \n",
        "          if cycle_num < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * cycle_num + lr_start   \n",
        "          elif cycle_num < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max    \n",
        "          else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(cycle_num - lr_ramp_ep - lr_sus_ep) + lr_min   \n",
        "          return lr\n",
        "def one_epoch(model, \n",
        "              criterion, \n",
        "              loader,\n",
        "              optimizer=None, \n",
        "              lr_scheduler=None, \n",
        "              mode=\"train\", \n",
        "              step=\"batch\", epoch=0, clr_iterations=0, swa_weights=None ,model_count=0):\n",
        "    \n",
        "    loss_meter = AvgMeter()\n",
        "    acc_meter = AvgMeter()\n",
        "    \n",
        "    \n",
        "    #########################################\n",
        "    iter_per_cycle=2*421\n",
        "    start_inx=40\n",
        "    lrs=[]\n",
        "    swa_cycle_start_inx=40\n",
        "    #############################################\n",
        "    \n",
        "    \n",
        "    \n",
        "    lrs=[]\n",
        "    lr=1e-5\n",
        "    tqdm_object = tqdm(loader, total=len(loader))\n",
        "    for batch in tqdm_object:\n",
        "    \n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n",
        "        \n",
        "        clr_iterations+=1\n",
        "        \n",
        "        if epoch>=swa_cycle_start_inx:\n",
        "            lr=_clr_schedule(1e-5,1e-6,clr_iterations,iter_per_cycle,epoch)\n",
        "            optimizer=set_value(optimizer,lr)\n",
        "        lrs.append(lr)\n",
        "        \n",
        "        \n",
        "        preds = model(batch)\n",
        "        loss = criterion(preds, batch['labels'])\n",
        "        if mode == \"train\":\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if step == \"batch\":\n",
        "                lr_scheduler.step()\n",
        "                \n",
        "        count = batch['input_ids'].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "        \n",
        "        accuracy = get_accuracy(preds.detach(), batch['labels'])\n",
        "        acc_meter.update(accuracy.item(), count)\n",
        "        if mode == \"train\":\n",
        "            tqdm_object.set_postfix(train_loss=loss_meter.avg, accuracy=acc_meter.avg, lr=get_lr(optimizer))\n",
        "        else:\n",
        "            tqdm_object.set_postfix(valid_loss=loss_meter.avg, accuracy=acc_meter.avg)\n",
        "    if (_t_cycle(clr_iterations,iter_per_cycle) !=1) or (epoch<start_inx):\n",
        "      return  loss_meter,acc_meter,clr_iterations,swa_weights,model_count,lrs\n",
        "    swa_weights=weight_update(model,model_count,swa_weights)\n",
        "    model_count+=1\n",
        "    return loss_meter, acc_meter, clr_iterations, swa_weights,model_count,lrs\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group[\"lr\"]\n",
        "\n",
        "def get_accuracy(preds, targets):\n",
        "    \"\"\"\n",
        "    preds shape: (batch_size, num_labels)\n",
        "    targets shape: (batch_size)\n",
        "    \"\"\"\n",
        "    preds = preds.argmax(dim=1)\n",
        "    acc = (preds == targets).float().mean()\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClpHiffnZFCf"
      },
      "source": [
        "def train_eval(epochs, model, train_loader, \n",
        "               criterion, optimizer, lr_scheduler=None):\n",
        "    \n",
        "    best_loss = float('inf')\n",
        "    best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    clr_iterations=0\n",
        "    cycle_num=0\n",
        "    model_count=0\n",
        "    lrs=[]\n",
        "    for epoch in range(epochs):\n",
        "        print(\"*\" * 30)\n",
        "        print(f\"Epoch {epoch + 1}\")\n",
        "        current_lr = get_lr(optimizer)\n",
        "        if epoch==0:\n",
        "            swa_weights=None\n",
        "        model.train()\n",
        "        train_loss, train_acc,clr_iterations,swa_weights,model_count,lr = one_epoch(model, \n",
        "                                          criterion, \n",
        "                                          train_loader, \n",
        "                                          optimizer=optimizer,\n",
        "                                          lr_scheduler=lr_scheduler,\n",
        "                                          mode=\"train\",\n",
        "                                          step=CFG.step, epoch=epoch,clr_iterations=clr_iterations\n",
        "                                        ,swa_weights=swa_weights,model_count=model_count)                     \n",
        "        model.eval()\n",
        "        lrs.append(lr)\n",
        "        print(\"*\" * 30)\n",
        "    return lrs\n",
        "train_dataset = TextDataset(train, tokenizer, max_length=CFG.max_length)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                           batch_size=CFG.batch_size, \n",
        "                                           num_workers=CFG.num_workers, \n",
        "                                           shuffle=True)\n",
        "\n",
        "model = Model(bert_model).to(CFG.device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)\n",
        "if CFG.scheduler == \"ReduceLROnPlateau\":\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
        "                                                              mode=\"min\", \n",
        "                                                              factor=CFG.factor, \n",
        "                                                              patience=CFG.patience)\n",
        "\n",
        "lrs=train_eval(60, model, train_loader,\n",
        "           criterion, optimizer, lr_scheduler=lr_scheduler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiMgtER0ZE_x"
      },
      "source": [
        "total=[]\n",
        "for i in lrs:\n",
        "    total+=i\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(total)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSx_yv17ZIe3"
      },
      "source": [
        "\n",
        "\n",
        "!mkdir tokenizer\n",
        "tokenizer.save_pretrained(\"./tokenizer\")\n",
        "bert_model.save_pretrained('abc')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}