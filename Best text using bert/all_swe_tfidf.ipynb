{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "all_swe_tfidf.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOjtOnCBVcRX+FPuVdmV+rL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/shopee/blob/main/all_swe_tfidf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxU7chZVTbLj"
      },
      "source": [
        "import os\n",
        "import copy\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.autonotebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import transformers\n",
        "from transformers import (BertTokenizer, BertModel,\n",
        "                          DistilBertTokenizer, DistilBertModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sLS0CSdY33i"
      },
      "source": [
        "import numpy as np, pandas as pd, gc\n",
        "import cv2, matplotlib.pyplot as plt\n",
        "import cudf, cuml, cupy\n",
        "from cuml.feature_extraction.text import TfidfVectorizer\n",
        "from cuml.neighbors import NearestNeighbors\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import *\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm.notebook import tqdm\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "print('RAPIDS',cuml.__version__)\n",
        "print('TF',tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahwfdhdhY30o"
      },
      "source": [
        "LIMIT = 1\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*LIMIT)])\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    print(e)\n",
        "print('We will restrict TensorFlow to max %iGB GPU RAM'%LIMIT)\n",
        "print('then RAPIDS can use %iGB GPU RAM'%(16-LIMIT))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih_ev6ToY3xn"
      },
      "source": [
        "COMPUTE_CV = True\n",
        "\n",
        "test = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
        "if len(test)>3: COMPUTE_CV = False\n",
        "else: print('this submission notebook will compute CV score, but commit notebook will not')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oee4oCeBY3u_"
      },
      "source": [
        "train = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
        "tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n",
        "train['target'] = train.label_group.map(tmp)\n",
        "print('train shape is', train.shape )\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VQ1AIcoY3sX"
      },
      "source": [
        "tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\n",
        "train['oof'] = train.image_phash.map(tmp)\n",
        "def getMetric(col):\n",
        "    def f1score(row):\n",
        "        n = len( np.intersect1d(row.target,row[col]) )\n",
        "        return 2*n / (len(row.target)+len(row[col]))\n",
        "    return f1score\n",
        "train['f1'] = train.apply(getMetric('oof'),axis=1)\n",
        "print('CV score for baseline =',train.f1.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiDHkSJ8Y3ps"
      },
      "source": [
        "if COMPUTE_CV:\n",
        "    test = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
        "    test_gf = cudf.DataFrame(test)\n",
        "    print('Using train as test to compute CV (since commit notebook). Shape is', test_gf.shape )\n",
        "else:\n",
        "    test = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
        "    test_gf = cudf.read_csv('../input/shopee-product-matching/test.csv')\n",
        "    print('Test shape is', test_gf.shape )\n",
        "test_gf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDd3h_E9Y3nC"
      },
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=None):\n",
        "        self.dataframe = dataframe\n",
        "        texts = list(dataframe['title'].apply(lambda o: str(o)).values)\n",
        "        self.encodings = tokenizer(texts, \n",
        "                                   padding=True, \n",
        "                                   truncation=True, \n",
        "                                   max_length=max_length)\n",
        "        \n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n",
        "        return item\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "NUM_CLASSES=test_gf['label_group'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOecOpPrZFFn"
      },
      "source": [
        "class CFG:\n",
        "    DistilBERT = True # if set to False, BERT model will be used\n",
        "    bert_hidden_size = 768\n",
        "    \n",
        "    batch_size = 64\n",
        "    epochs = 30\n",
        "    num_workers = 4\n",
        "    learning_rate = 1e-5 #3e-5\n",
        "    scheduler = \"ReduceLROnPlateau\"\n",
        "    step = 'epoch'\n",
        "    patience = 2\n",
        "    factor = 0.8\n",
        "    dropout = 0.5\n",
        "    model_path = \"/kaggle/working\"\n",
        "    max_length = 120\n",
        "    model_save_name = \"model.pt\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClpHiffnZFCf"
      },
      "source": [
        "\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    r\"\"\"Implement of large margin arc distance: :\n",
        "        Args:\n",
        "            in_features: size of each input sample\n",
        "            out_features: size of each output sample\n",
        "            s: norm of input feature\n",
        "            m: margin\n",
        "            cos(theta + m)\n",
        "        \"\"\"\n",
        "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
        "        super(ArcMarginProduct, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = math.cos(m)\n",
        "        self.sin_m = math.sin(m)\n",
        "        self.th = math.cos(math.pi - m)\n",
        "        self.mm = math.sin(math.pi - m) * m\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
        "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
        "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        # --------------------------- convert label to one-hot ---------------------------\n",
        "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
        "        one_hot = torch.zeros(cosine.size(), device=CFG.device)\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
        "        output *= self.s\n",
        "        # print(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('../input/all-swe/tokenizer')\n",
        "bert_model = DistilBertModel.from_pretrained('../input/all-swe/abc').to('cuda')\n",
        "# WGT = '../input/shopee-efficientnetb6-arcmarginproduct/EfficientNetB6_512_42.h5'\n",
        "# model = efn.EfficientNetB6(weights=WGT,include_top=False, pooling='avg', input_shape=None)\n",
        "\n",
        "embeds = []\n",
        "CHUNK = 1024*4\n",
        "\n",
        "print('Computing text embeddings...')\n",
        "CTS = len(test)//CHUNK\n",
        "if len(test)%CHUNK!=0: CTS += 1\n",
        "for i,j in enumerate( range( CTS ) ):\n",
        "    \n",
        "    a = j*CHUNK\n",
        "    b = (j+1)*CHUNK\n",
        "    b = min(b,len(test))\n",
        "    print('chunk',a,'to',b)\n",
        "    train_dataset = TextDataset(test.iloc[a:b], tokenizer, max_length=CFG.max_length)\n",
        "    test_gen = torch.utils.data.DataLoader(train_dataset, \n",
        "                                           batch_size=CFG.batch_size, \n",
        "                                           num_workers=CFG.num_workers, \n",
        "                                           shuffle=False)\n",
        "    image_embeddings = []\n",
        "    tqdm_object = tqdm(test_gen, total=len(test_gen))\n",
        "    for batch in tqdm_object:\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n",
        "        image_embeddings.append(bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask']).last_hidden_state[:,0,:].detach().cpu())\n",
        "    embeds.append(image_embeddings)\n",
        "    del test_gen\n",
        "    _ = gc.collect()\n",
        "    #if i>=1: break\n",
        "    \n",
        "del bert_model\n",
        "_ = gc.collect()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiMgtER0ZE_x"
      },
      "source": [
        "\n",
        "\n",
        "ls=[]\n",
        "for en,i in enumerate(embeds):\n",
        "    if en==0:\n",
        "        a=torch.cat(i)\n",
        "    else:\n",
        "        a=torch.cat(i)\n",
        "    ls.append(a)   \n",
        "image_embeddings=torch.cat(ls).numpy()\n",
        "\n",
        "KNN = 50\n",
        "if len(test)==3: KNN = 2\n",
        "model = NearestNeighbors(n_neighbors=KNN,metric='cosine')\n",
        "model.fit(image_embeddings)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSx_yv17ZIe3"
      },
      "source": [
        "preds = []\n",
        "CHUNK = 1024*4\n",
        "\n",
        "print('Finding similar images...')\n",
        "CTS = len(image_embeddings)//CHUNK\n",
        "if len(image_embeddings)%CHUNK!=0: CTS += 1\n",
        "for j in range( CTS ):\n",
        "    \n",
        "    a = j*CHUNK\n",
        "    b = (j+1)*CHUNK\n",
        "    b = min(b,len(image_embeddings))\n",
        "    print('chunk',a,'to',b)\n",
        "    distances, indices = model.kneighbors(image_embeddings[a:b,])\n",
        "    \n",
        "    for k in range(b-a):\n",
        "        IDX = np.where(distances[k,]<0.32)[0]\n",
        "        IDS = indices[k,IDX]\n",
        "        o = test.iloc[IDS].posting_id.values\n",
        "        preds.append(o)\n",
        "        \n",
        "# del model, distances, indices, image_embeddings, embeds\n",
        "# _ = gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CqE7HMYZIcK"
      },
      "source": [
        "\n",
        "\n",
        "test['preds2'] = preds\n",
        "test.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6eCy5xNZIZk"
      },
      "source": [
        "\n",
        "\n",
        "tmp = test.groupby('image_phash').posting_id.agg('unique').to_dict()\n",
        "test['preds3'] = test.image_phash.map(tmp)\n",
        "test.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_kuKnimZLEf"
      },
      "source": [
        "\n",
        "\n",
        "print('Computing text embeddings...')\n",
        "model = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\n",
        "text_embeddings = model.fit_transform(test_gf.title).toarray()\n",
        "print('text embeddings shape',text_embeddings.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOpxMjgiZ8i2"
      },
      "source": [
        "preds = []\n",
        "CHUNK = 1024*4\n",
        "\n",
        "print('Finding similar titles...')\n",
        "CTS = len(test)//CHUNK\n",
        "if len(test)%CHUNK!=0: CTS += 1\n",
        "for j in range( CTS ):\n",
        "    \n",
        "    a = j*CHUNK\n",
        "    b = (j+1)*CHUNK\n",
        "    b = min(b,len(test))\n",
        "    print('chunk',a,'to',b)\n",
        "    \n",
        "    # COSINE SIMILARITY DISTANCE\n",
        "    cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n",
        "    \n",
        "    for k in range(b-a):\n",
        "        IDX = cupy.where(cts[k,]>0.7)[0]\n",
        "        o = test.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
        "        preds.append(o)\n",
        "        \n",
        "del model, text_embeddings\n",
        "_ = gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXrCN-NvZ8f-"
      },
      "source": [
        "test['preds'] = preds\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEY708azZ8dj"
      },
      "source": [
        "def combine_for_sub(row):\n",
        "    x = list(set(row.preds2).union(row.preds3).union(row.preds))\n",
        "    return ' '.join( np.unique(x) )\n",
        "\n",
        "def combine_for_cv(row):\n",
        "    x = list(set(row.preds2).union(row.preds3).union(row.preds3))\n",
        "    return np.unique(x)\n",
        "if COMPUTE_CV:\n",
        "    tmp = test.groupby('label_group').posting_id.agg('unique').to_dict()\n",
        "    test['target'] = test.label_group.map(tmp)\n",
        "    test['oof'] = test.apply(combine_for_cv,axis=1)\n",
        "    test['f1'] = test.apply(getMetric('oof'),axis=1)\n",
        "    print('CV Score =', test.f1.mean() )\n",
        "\n",
        "test['matches'] = test.apply(combine_for_sub,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu0ABTLiZ8aw"
      },
      "source": [
        "test[['posting_id','matches']].to_csv('submission.csv',index=False)\n",
        "sub = pd.read_csv('submission.csv')\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
