{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_dec",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/shopee/blob/main/bert_dec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQqlrXIJej1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb9cdb7-eb31-446d-dbf2-b91e8b90e4df"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WXDyhihenRg"
      },
      "source": [
        "from shutil import copyfile\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ_0F8Zfep7F"
      },
      "source": [
        "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wGAa3rSglXh",
        "outputId": "09b91869-1a09-42fa-f558-65447403d4a0"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.1MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PRGrVe9glUZ"
      },
      "source": [
        "import tokenization"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-dKOrmaglRI"
      },
      "source": [
        "# Configuration\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 32\n",
        "# Seed\n",
        "SEED = 123\n",
        "# Verbosity\n",
        "VERBOSE = 1\n",
        "LR = 5e-5"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OauHZNZMerDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cde4d97-1699-463b-f3f5-df353565096f"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "def read_and_preprocess():\n",
        "    df = pd.read_csv('/content/gdrive/My Drive/shopee/train.csv.zip')\n",
        "    tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
        "    df['matches'] = df['label_group'].map(tmp)\n",
        "    df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
        "    encoder = LabelEncoder()\n",
        "    df['label_group'] = encoder.fit_transform(df['label_group'])\n",
        "    N_CLASSES = df['label_group'].nunique()\n",
        "    print(f'We have {N_CLASSES} classes')\n",
        "    x_train, x_val, y_train, y_val = train_test_split(df[['title']], df['label_group'], shuffle = True, stratify = df['label_group'], random_state = SEED, test_size = 0.33)\n",
        "    return df, N_CLASSES, x_train, x_val, y_train, y_val\n",
        "\n",
        "# Return tokens, masks and segments from a text array or series\n",
        "def bert_encode(texts, tokenizer, max_len=512):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "            \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "        tokens += [0] * pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "        segment_ids = [0] * max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "    \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
        "\n",
        "\n",
        "# Arcmarginproduct class keras layer\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y = inputs\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        return output\n",
        "\n",
        "# Function to build bert model\n",
        "def build_bert_model(bert_layer, max_len = 512):\n",
        "    \n",
        "    margin = ArcMarginProduct(\n",
        "            n_classes = N_CLASSES, \n",
        "            s = 30, \n",
        "            m = 0.5, \n",
        "            name='head/arc_margin', \n",
        "            dtype='float32'\n",
        "            )\n",
        "    \n",
        "    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "    label = tf.keras.layers.Input(shape = (), name = 'label')\n",
        "\n",
        "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "    clf_output = sequence_output[:, 0, :]\n",
        "    x = Dropout(0.3)(clf_output)\n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = PReLU()(x)\n",
        "    x = margin([x, label])\n",
        "    output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = LR),\n",
        "                  loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "                  metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "    return model\n",
        "\n",
        "def load_train_and_evaluate(x_train, x_val, y_train, y_val):\n",
        "    seed_everything(SEED)\n",
        "    module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
        "    bert_layer = hub.KerasLayer(module_url, trainable=True)\n",
        "\n",
        "    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "    x_train = bert_encode(x_train['title'].values, tokenizer, max_len = 70)\n",
        "    x_val = bert_encode(x_val['title'].values, tokenizer, max_len = 70)\n",
        "    y_train = y_train.values\n",
        "    y_val = y_val.values\n",
        "    # Add targets to train and val\n",
        "    x_train = (x_train[0], x_train[1], x_train[2], y_train)\n",
        "    x_val = (x_val[0], x_val[1], x_val[2], y_val)\n",
        "    bert_model = build_bert_model(bert_layer, max_len = 70)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint('bert_dec.hdf5', \n",
        "                                                    monitor = 'val_loss', \n",
        "                                                    verbose = VERBOSE, \n",
        "                                                    save_best_only = True,\n",
        "                                                    save_weights_only = True, \n",
        "                                                    mode = 'min')\n",
        "    def get_lr_callback():\n",
        "   \n",
        "        def lrfn(epoch):\n",
        "            if epoch < 5:\n",
        "                lr = 1e-4  \n",
        "            elif epoch < 7:\n",
        "                lr = 5e-5  \n",
        "            elif epoch < 10:\n",
        "                lr = 3e-5  \n",
        "            elif epoch > 10:\n",
        "                lr = 1e-5    \n",
        "            return lr\n",
        "\n",
        "        lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
        "        return lr_callback\n",
        "    history = bert_model.fit(x_train, y_train,\n",
        "                             validation_data = (x_val, y_val),\n",
        "                             epochs = EPOCHS, \n",
        "                             callbacks = [checkpoint,get_lr_callback()],\n",
        "                             batch_size = BATCH_SIZE,\n",
        "                             verbose = VERBOSE)\n",
        "    \n",
        "    \n",
        "\n",
        "df, N_CLASSES, x_train, x_val, y_train, y_val = read_and_preprocess()\n",
        "load_train_and_evaluate(x_train, x_val, y_train, y_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 11014 classes\n",
            "Epoch 1/25\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "718/718 [==============================] - 1232s 2s/step - loss: 23.8921 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 21.1671 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 21.16710, saving model to /content/gdrive/MyDrive/shopee/bert_dec.hdf5\n",
            "Epoch 2/25\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "718/718 [==============================] - 1208s 2s/step - loss: 20.7097 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 19.6845 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00002: val_loss improved from 21.16710 to 19.68454, saving model to /content/gdrive/MyDrive/shopee/bert_dec.hdf5\n",
            "Epoch 3/25\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "718/718 [==============================] - 1209s 2s/step - loss: 19.5585 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 18.2200 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00003: val_loss improved from 19.68454 to 18.21996, saving model to /content/gdrive/MyDrive/shopee/bert_dec.hdf5\n",
            "Epoch 4/25\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "718/718 [==============================] - 1208s 2s/step - loss: 16.6719 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 16.6252 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00004: val_loss improved from 18.21996 to 16.62521, saving model to /content/gdrive/MyDrive/shopee/bert_dec.hdf5\n",
            "Epoch 5/25\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "718/718 [==============================] - 1206s 2s/step - loss: 15.1189 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 18.3735 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 16.62521\n",
            "Epoch 6/25\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 5e-05.\n",
            "718/718 [==============================] - 1209s 2s/step - loss: 15.4056 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 16.8050 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "718/718 [==============================] - 1209s 2s/step - loss: 15.4056 - sparse_categorical_accuracy: 0.0000e+00 - val_loss: 16.8050 - val_sparse_categorical_accuracy: 0.0000e+00\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 16.62521\n",
            "Epoch 7/25\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 5e-05.\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 16.62521\n",
            "Epoch 7/25\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 5e-05.\n",
            "173/718 [======>.......................] - ETA: 12:58 - loss: 15.1372 - sparse_categorical_accuracy: 0.0000e+00"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOdQm2sOWzCv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}