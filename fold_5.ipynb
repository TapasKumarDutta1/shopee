{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fold_5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM1trcnTytpaObyjoegQlM8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyhound101/shopee/blob/main/fold_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxU7chZVTbLj"
      },
      "source": [
        "!pip install -q efficientnet\n",
        "!pip install tensorflow_addons\n",
        "import re\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import efficientnet.tfkeras as efn\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from tqdm.notebook import tqdm\n",
        "from kaggle_datasets import KaggleDatasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2SSk-wIUxtK"
      },
      "source": [
        "try:\n",
        "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
        "    # set: this is always the case on Kaggle.\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "\n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "else:\n",
        "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD7Yd6gWUxqZ"
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Data access\n",
        "GCS_PATH = KaggleDatasets().get_gcs_path('fold5all')\n",
        "\n",
        "# Configuration\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
        "IMAGE_SIZE = [512,512]\n",
        "# Seed\n",
        "SEED = 42\n",
        "# Learning rate\n",
        "LR = 0.001\n",
        "# Verbosity\n",
        "VERBOSE = 2\n",
        "# Number of classes\n",
        "N_CLASSES = 11014\n",
        "# Number of folds\n",
        "FOLDS = 5\n",
        "\n",
        "# Training filenames directory\n",
        "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH+'/*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lx5YkOp5Uxng"
      },
      "source": [
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Data access\n",
        "GCS_PATH = KaggleDatasets().get_gcs_path('fold5all')\n",
        "\n",
        "# Configuration\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
        "IMAGE_SIZE = [512,512]\n",
        "# Seed\n",
        "SEED = 42\n",
        "# Learning rate\n",
        "LR = 0.001\n",
        "# Verbosity\n",
        "VERBOSE = 2\n",
        "# Number of classes\n",
        "# Number of folds\n",
        "FOLDS = 5\n",
        "\n",
        "# Training filenames directory\n",
        "TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH+'/*')\n",
        "\n",
        "# Function to get our f1 score\n",
        "def f1_score(y_true, y_pred):\n",
        "    y_true = y_true.apply(lambda x: set(x.split()))\n",
        "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
        "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
        "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
        "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
        "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
        "    return f1\n",
        "\n",
        "# Function to seed everything\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "def arcface_format(image, target,count,margin):\n",
        "    return {'inp1': image, 'inp2': target, 'inp3': margin},target\n",
        "\n",
        "# Data augmentation function\n",
        "def data_augment(image, target,count,margin):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_hue(image, 0.01)\n",
        "    image = tf.image.random_saturation(image, 0.70, 1.30)\n",
        "    image = tf.image.random_contrast(image, 0.80, 1.20)\n",
        "    image = tf.image.random_brightness(image, 0.10)\n",
        "    target = tf.reshape(target,(-1,1))\n",
        "    margin = tf.reshape(margin,(-1,1))\n",
        "    return image, target,count,margin\n",
        "\n",
        "# Function to decode our images\n",
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels = 3)\n",
        "    image = tf.image.resize(image, IMAGE_SIZE)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image\n",
        "\n",
        "# This function parse our images and also get the target variable\n",
        "def read_labeled_tfrecord(example):\n",
        "    LABELED_TFREC_FORMAT = {\n",
        "        \"ocr\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"margin\": tf.io.FixedLenFeature([], tf.float32),\n",
        "        \"count\": tf.io.FixedLenFeature([], tf.float32)\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
        "    image = decode_image(example['ocr'])\n",
        "    count = tf.cast(example['count'], tf.float32)\n",
        "    margin = tf.cast(example['margin'], tf.float32)\n",
        "#     label_group = tf.one_hot(tf.cast(example['label_group'], tf.int32), depth = N_CLASSES)\n",
        "    target = tf.cast(example['target'], tf.int32)\n",
        "    return image, target,count,margin\n",
        "\n",
        "# This function loads TF Records and parse them into tensors\n",
        "def load_dataset(filenames, ordered = False):\n",
        "    \n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False \n",
        "        \n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n",
        "    dataset = dataset.with_options(ignore_order)\n",
        "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls = AUTO) \n",
        "    return dataset\n",
        "\n",
        "# This function is to get our training tensors\n",
        "def get_training_dataset(filenames, ordered = False):\n",
        "    dataset = load_dataset(filenames, ordered = ordered)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO)\n",
        "    return dataset\n",
        "\n",
        "# This function is to get our validation tensors\n",
        "def get_validation_dataset(filenames, ordered = True):\n",
        "    dataset = load_dataset(filenames, ordered = ordered)\n",
        "    dataset = dataset.map(arcface_format, num_parallel_calls = AUTO)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) \n",
        "    return dataset\n",
        "\n",
        "# Function to count how many photos we have in\n",
        "def count_data_items(filenames):\n",
        "    counts={'1':8793,'2':8804,'3':8811,'4':8819,'5':8829}\n",
        "    total={'1':26937,'2':27270,'3':27457,'4':27589,'5':27747}\n",
        "    n = [total[filename.split('.')[0].split('_')[-1]] for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "def count_targets(filenames):\n",
        "    counts={'1':8793,'2':8804,'3':8811,'4':8819,'5':8829}\n",
        "    total={'1':26937,'2':27270,'3':27457,'4':27589,'5':27747}\n",
        "    n = [counts[filename.split('.')[0].split('_')[-1]] for filename in filenames]\n",
        "    return np.sum(n)\n",
        "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
        "print(f'Dataset: {NUM_TRAINING_IMAGES} training images')\n",
        "N_CLASSES =  count_targets(TRAINING_FILENAMES)\n",
        "print(f'Dataset: {N_CLASSES} different training images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HHIm8MqUxkh"
      },
      "source": [
        "def get_lr_callback():\n",
        "    lr_start   = 0.000001\n",
        "    lr_max     = 0.000005 * BATCH_SIZE\n",
        "    lr_min     = 0.000001\n",
        "    lr_ramp_ep = 5\n",
        "    lr_sus_ep  = 0\n",
        "    lr_decay   = 0.8\n",
        "   \n",
        "    def lrfn(epoch):\n",
        "        if epoch < lr_ramp_ep:\n",
        "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start   \n",
        "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
        "            lr = lr_max    \n",
        "        else:\n",
        "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min    \n",
        "        return lr\n",
        "\n",
        "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
        "    return lr_callback\n",
        "\n",
        "# Arcmarginproduct class keras layer\n",
        "class ArcMarginProduct(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    Implements large margin arc distance.\n",
        "\n",
        "    Reference:\n",
        "        https://arxiv.org/pdf/1801.07698.pdf\n",
        "        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
        "            blob/master/src/modeling/metric_learning.py\n",
        "    '''\n",
        "    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
        "                 ls_eps=0.0, **kwargs):\n",
        "\n",
        "        super(ArcMarginProduct, self).__init__(**kwargs)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.ls_eps = ls_eps\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'n_classes': self.n_classes,\n",
        "            's': self.s,\n",
        "            'm': self.m,\n",
        "            'ls_eps': self.ls_eps,\n",
        "            'easy_margin': self.easy_margin,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(ArcMarginProduct, self).build(input_shape[0])\n",
        "\n",
        "        self.W = self.add_weight(\n",
        "            name='W',\n",
        "            shape=(int(input_shape[0][-1]), self.n_classes),\n",
        "            initializer='glorot_uniform',\n",
        "            dtype='float32',\n",
        "            trainable=True,\n",
        "            regularizer=None)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        X, y,dm = inputs\n",
        "        y = tf.reshape(y,(-1,1))\n",
        "        y = tf.cast(y, dtype=tf.int32)\n",
        "        m = tf.reshape(dm,(-1,1))\n",
        "        self.m = m\n",
        "        self.cos_m = tf.math.cos(m)\n",
        "        self.sin_m = tf.math.sin(m)\n",
        "        self.th = tf.math.cos(math.pi - m)\n",
        "        self.mm = tf.math.sin(math.pi - m) * m\n",
        "        cosine = tf.matmul(\n",
        "            tf.math.l2_normalize(X, axis=1),\n",
        "            tf.math.l2_normalize(self.W, axis=0)\n",
        "        )\n",
        "        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "        if self.easy_margin:\n",
        "            phi = tf.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
        "        one_hot = tf.cast(\n",
        "            tf.one_hot(y, depth=self.n_classes),\n",
        "            dtype=cosine.dtype\n",
        "        )\n",
        "        one_hot=one_hot[:,0,:]\n",
        "        if self.ls_eps > 0:\n",
        "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
        "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "        outpu = tf.reshape(output,(-1,N_CLASSES))\n",
        "        return output\n",
        "\n",
        "class GeMPoolingLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, p=1., train_p=False):\n",
        "        super().__init__()\n",
        "        self.p = tf.Variable(p, dtype=tf.float32)\n",
        "        self.eps = 1e-6\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, **kwargs):\n",
        "        inputs = tf.clip_by_value(inputs, clip_value_min=1e-6, clip_value_max=tf.reduce_max(inputs))\n",
        "        inputs = tf.pow(inputs, self.p)\n",
        "        inputs = tf.reduce_mean(inputs, axis=[1, 2], keepdims=False)\n",
        "        inputs = tf.pow(inputs, 1./self.p)\n",
        "        return inputs\n",
        "\n",
        "# Function to create our EfficientNetB3 model\n",
        "def get_model():\n",
        "\n",
        "    with strategy.scope():\n",
        "\n",
        "        margin = ArcMarginProduct(\n",
        "            n_classes = N_CLASSES, \n",
        "            s = 30, \n",
        "            m = 0.5, \n",
        "            name='head/arc_margin', \n",
        "            dtype='float32'\n",
        "            )\n",
        "\n",
        "        inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n",
        "        label = tf.keras.layers.Input(shape = (), name = 'inp2')\n",
        "        margin1 = tf.keras.layers.Input(shape = (), name = 'inp3')\n",
        "        x = efn.EfficientNetB0(weights = 'imagenet', include_top = False)(inp)\n",
        "        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "#         x = tf.keras.layers.Dropout(0.25)(x)\n",
        "#         x = tf.keras.layers.Dense(512)(x)\n",
        "#         x = tf.keras.layers.BatchNormalization()(x)\n",
        "#         x = tf.keras.layers.PReLU()(x)\n",
        "        x = margin([x, label,margin1])\n",
        "        \n",
        "        output = tf.keras.layers.Softmax(dtype='float32')(x)\n",
        "\n",
        "        model = tf.keras.models.Model(inputs = [inp, label,margin1], outputs = [output])\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer = opt,\n",
        "            loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n",
        "            metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        "            ) \n",
        "        \n",
        "        return model\n",
        "\n",
        "def train_and_evaluate():\n",
        "\n",
        "    # Seed everything\n",
        "    seed_everything(SEED)\n",
        "    \n",
        "    print('\\n')\n",
        "    print('-'*50)\n",
        "    train=TRAINING_FILENAMES\n",
        "    train_dataset = get_training_dataset(TRAINING_FILENAMES, ordered = False)\n",
        "    \n",
        "    STEPS_PER_EPOCH = count_data_items(train) // BATCH_SIZE\n",
        "    STEPS_PER_EPOCH_all = count_data_items(TRAINING_FILENAMES) // BATCH_SIZE\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = get_model()\n",
        "    history = model.fit(train_dataset,\n",
        "                        steps_per_epoch = STEPS_PER_EPOCH,\n",
        "                        epochs = 20,\n",
        "                        callbacks = [get_lr_callback()])\n",
        "\n",
        "    model.save_weights('weights.hdf5')\n",
        "train_and_evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}